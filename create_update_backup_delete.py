# ============================================================
# Script: create_update_backup_delete.py
# Author: [BruceBanner001]
# Description:
#   This script automates the creation, update, and backup process
#   for JSON data objects derived from Excel or YAML workflows.
#
#   Key features:
#   - One backup per workflow run (contains only modified objects).
#   - Intelligent field merging (preserves 'otherNames', etc. when incoming empty).
#   - Skipped detection for unchanged records.
#   - Detailed reporting with per-field change summaries.
#   - Clean, scalable structure with clear comments.
#
# ============================================================


# ============================================================================
# Patched Script: create_update_backup_delete.py
# Purpose: Excel -> JSON automation (patched for enhanced synopsis/image fetching,
#          deletion handling, and single-email composing for CI workflows).
#
# IMPORTANT NOTES FOR MAINTENANCE / EXTENSION
# -------------------------------------------------
# 1) Site Preferences / Language mapping:
#    - The code uses a simple mapping to decide "preferred sites" for fetching synopsis
#      and images based on the show's native language (nativeLanguage field).
#    - To add a new language preference:
#        a) Locate the function excel_to_objects(...) and find the block that sets `prefer`.
#        b) Add a new branch for the language name (use lowercased checks, and include
#           possible variants, e.g. 'korean', 'korea', 'korean language').
#        c) Update fetch_synopsis_and_duration(...) and fetch_and_save_image_for_show(...)
#           if you want to treat the new site specially (site-specific parsing).
#
# 2) Adding a new preferred site parser:
#    - If you want more accurate extraction from a particular site (e.g., 'asianwiki' or 'mydramalist'),
#      add a new branch in parse_synopsis_from_html(...) that checks the domain and applies
#      site-specific DOM selectors (e.g., look for elements with id/class 'synopsis', 'summary',
#      or meta property 'og:description'). Keep a generic fallback for robustness.
#
# 3) Email behavior for CI (GitHub Actions):
#    - This script now composes the full email body in-memory via compose_email_body_from_report(report_path).
#      We purposely **do not** write email_body_*.txt files to disk anymore.
#    - The workflow should either:
#        A) read reports/report_*.txt and send email with that content, or
#        B) run this script and capture the printed email body (stdout) and pass it to the email action.
#    - The subject format required by the owner: "[Manual] Workflow <DD Month YY HHMM> Report"
#
# 4) Deletion behavior:
#    - When a showID is deleted via the "Deleting Records" sheet:
#        * the deleted object is saved to deleted-data/DELETED_<timestamp>_<id>.json
#        * the associated image file (if present under images/) is moved to old-images/
#        * a report entry is generated for moved images: 'deleted_images_moved'
#
# 5) Synopsis length:
#    - Controlled by SYNOPSIS_MAX_LEN environment variable (default 1500). Soft truncation attempts to cut at sentence end.
#
# 6) Debugging:
#    - Set DEBUG_FETCH=true in env to print useful debug messages.
#
# ============================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Script: create_update_backup_delete.py
# Purpose: Excel -> JSON automation (patched to not require a local Excel file).
#
# Requirements (recommended):
#   pip install pandas requests beautifulsoup4 pillow openpyxl google-api-python-client google-auth-httplib2 google-auth
# Notes:
#   - The script expects two files to be present in the runner environment:
#       EXCEL_FILE_ID.txt         (text file containing the Google Drive file id)
#       GDRIVE_SERVICE_ACCOUNT.json  (service account JSON key)
#   - If google-api-python-client/google-auth packages are not available, script
#     will exit gracefully with instructions (no hard crash).
# ============================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
create_update_backup_delete.py — v2.0.0 (Stable)
Maintains series data by fetching, updating, and backing up metadata and images.

Author: Generated by assistant (adapted from user's original script)
Last Updated: autogenerated
Notes:
 - This script is a careful, reordered version of the user's original script with
   enhancements: site-priority, grouped fetch blocks, metadata backups, 1-time fetching rules,
   enhanced report format, scriptVersion tagging, and cleanup of old backups.
 - Keep credentials and EXCEL_FILE_ID.txt as in the original workflow.
"""

# --------------------------- VERSION & SITE PRIORITY ------------------------
SCRIPT_VERSION = "v2.5.0 (LockedFields & Report+)"

# SITE_PRIORITY_BY_LANGUAGE controls which site is preferred for each fetched property
SITE_PRIORITY_BY_LANGUAGE = {
    "korean": {
        "synopsis": "asianwiki",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "asianwiki"
    },
    "chinese": {
        "synopsis": "mydramalist",
        "image": "mydramalist",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "japanese": {
        "synopsis": "asianwiki",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "asianwiki"
    },
    "thai": {
        "synopsis": "mydramalist",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "taiwanese": {
        "synopsis": "mydramalist",
        "image": "mydramalist",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "default": {
        "synopsis": "mydramalist",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "asianwiki"
    }
}


# ---------------------------- Human-readable field name mapping ----------------
FIELD_NAME_MAP = {
    "showName": "Show Name",
    "showImage": "Show Image",
    "otherNames": "Other Names",
    "watchStartedOn": "Watch Started On",
    "watchEndedOn": "Watch Ended On",
    "releasedYear": "Released Year",
    "releaseDate": "Release Date",
    "totalEpisodes": "Total Episodes",
    "showType": "Show Type",
    "nativeLanguage": "Native Language",
    "watchedLanguage": "Watched Language",
    "country": "Country",
    "comments": "Comments",
    "ratings": "Ratings",
    "genres": "Category",
    "network": "Network",
    "againWatchedDates": "Again Watched Dates",
    "updatedOn": "Updated On",
    "updatedDetails": "Updated Details",
    "synopsis": "Synopsis",
    "topRatings": "Top Ratings",
    "Duration": "Duration",
    "sitePriorityUsed": "Site Priority Used",
    "sourceSites": "Source Sites"
}

def human_readable_field(field):
    if not field:
        return field
    if field in FIELD_NAME_MAP:
        return FIELD_NAME_MAP[field]
    # split camelCase or snake_case
    parts = re.sub(r'[_\-]+', ' ', field)
    parts = re.sub(r'([a-z])([A-Z])', r'\1 \2', parts)
    parts = parts.split()
    return " ".join([p.capitalize() for p in parts])

# ---------------------------- Imports & Globals ----------------------------
import os
import re
import sys
import time
import json
import io
import shutil
import traceback
import copy
from datetime import datetime, timedelta, timezone

import pandas as pd
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

try:
    from ddgs import DDGS
    HAVE_DDGS = True
except Exception:
    HAVE_DDGS = False

# Try Google APIs optionally (same as original)
try:
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload, HttpRequest
    HAVE_GOOGLE_API = True
except Exception:
    HAVE_GOOGLE_API = False

# Timezone: IST (match original helper)
IST = timezone(timedelta(hours=5, minutes=30))

def now_ist():
    return datetime.now(IST)

def filename_timestamp():
    return now_ist().strftime("%d_%B_%Y_%H%M")

# Paths and config (preserve original values)
JSON_FILE = "seriesData.json"
BACKUP_DIR = "backups"
IMAGES_DIR = "images"
DELETE_IMAGES_DIR = "deleted-images"
DELETED_DATA_DIR = "deleted-data"
REPORTS_DIR = "reports"
PROGRESS_DIR = ".progress"
PROGRESS_FILE = os.path.join(PROGRESS_DIR, "progress.json")
STATUS_JSON = os.path.join(REPORTS_DIR, "status.json")
BACKUP_META_DIR = "backup-meta-data"

EXCEL_FILE_ID_TXT = "EXCEL_FILE_ID.txt"
SERVICE_ACCOUNT_FILE = "GDRIVE_SERVICE_ACCOUNT.json"

GITHUB_PAGES_URL = os.environ.get("GITHUB_PAGES_URL", "").strip() or "https://<your-username>.github.io/my-movie-database"
MAX_PER_RUN = int(os.environ.get("MAX_PER_RUN", "0") or 0)
MAX_RUN_TIME_MINUTES = int(os.environ.get("MAX_RUN_TIME_MINUTES", "0") or 0)
KEEP_OLD_IMAGES_DAYS = int(os.environ.get("KEEP_OLD_IMAGES_DAYS", "7") or 7)
SCHEDULED_RUN = os.environ.get("SCHEDULED_RUN", "false").lower() == "true"
DEBUG_FETCH = os.environ.get("DEBUG_FETCH", "false").lower() == "true"
SYNOPSIS_MAX_LEN = int(os.environ.get("SYNOPSIS_MAX_LEN", "1000") or 1000)
METADATA_BACKUP_RETENTION_DAYS = int(os.environ.get("METADATA_BACKUP_RETENTION_DAYS", "90") or 90)

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; Bot/1.0)"}

# Fields that should NOT be changed automatically after initial creation
LOCKED_FIELDS_AFTER_CREATION = {
    'synopsis', 'showImage', 'otherNames', 'releaseDate', 'Duration',
    'updatedOn', 'updatedDetails', 'sitePriorityUsed', 'topRatings', 'sourceSites'
}

def logd(msg):
    if DEBUG_FETCH:
        print("[DEBUG]", msg)

# ---------------------------- Utilities -------------------------------------
def safe_filename(name):
    return re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())

def ddmmyyyy(val):
    if pd.isna(val):
        return None
    if isinstance(val, pd.Timestamp):
        return val.strftime("%d-%m-%Y")
    s = str(val).strip()
    try:
        dt = pd.to_datetime(s, dayfirst=True, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.strftime("%d-%m-%Y")
    except Exception:
        return None

def load_progress():
    os.makedirs(PROGRESS_DIR, exist_ok=True)
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_progress(progress):
    os.makedirs(PROGRESS_DIR, exist_ok=True)
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, indent=2)

def normalize_whitespace_and_sentences(s):
    if not s:
        return s
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"\.([^\s])", r". \1", s)
    return s

def normalize_list_from_csv(cell_value, cap=False, strip=False):
    if cell_value is None:
        return []
    if isinstance(cell_value, (list, tuple)):
        items = [str(x) for x in cell_value if x is not None and str(x).strip()]
    else:
        s = str(cell_value)
        if not s.strip():
            return []
        items = [p for p in [p.strip() for p in s.split(",")] if p]
    if cap:
        items = [p.capitalize() if p else p for p in items]
    if strip:
        items = [p.strip() for p in items]
    return items


# --- Helper: compare objects ignoring non-meaningful keys ---
def objects_differ(old, new, ignore_keys=None):
    if ignore_keys is None:
        ignore_keys = set(['updatedOn', 'updatedDetails', 'topRatings'])
    else:
        ignore_keys = set(ignore_keys)
    ks = set(old.keys() if isinstance(old, dict) else []) | set(new.keys() if isinstance(new, dict) else [])
    for k in ks:
        if k in ignore_keys:
            continue
        o = old.get(k) if isinstance(old, dict) else None
        n = new.get(k) if isinstance(new, dict) else None
        if isinstance(o, list) and isinstance(n, list):
            if sorted([str(x) for x in o]) != sorted([str(x) for x in n]):
                return True
            else:
                continue
        if o != n:
            return True
    return False


# ---------------------------- Date helpers ---------------------------------
_MONTHS = {m.lower(): m for m in ["January", "February", "March", "April", "May", "June",
                                  "July", "August", "September", "October", "November", "December"]}
_SHORT_MONTHS = {m[:3].lower(): m for m in _MONTHS}

def _normalize_month_name(m):
    mk = m.strip().lower()
    if mk in _MONTHS:
        return _MONTHS[mk]
    if mk in _SHORT_MONTHS:
        return _SHORT_MONTHS[mk]
    return m.capitalize()

def format_date_str(s):
    if not s:
        return None
    s = s.strip()
    m = re.search(r"([A-Za-z]+)\s+(\d{1,2}),\s*(\d{4})", s)
    if m:
        month = _normalize_month_name(m.group(1))
        day = str(int(m.group(2)))
        year = m.group(3)
        return f"{day} {month} {year}"
    m2 = re.search(r"(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})", s)
    if m2:
        day = str(int(m2.group(1)))
        month = _normalize_month_name(m2.group(2))
        year = m2.group(3)
        return f"{day} {month} {year}"
    return None

def format_date_range(s):
    if not s:
        return None
    m = re.search(r"([A-Za-z0-9,\s]+?)\s*[\-–]\s*([A-Za-z0-9,\s]+)", s)
    if m:
        d1 = format_date_str(m.group(1))
        d2 = format_date_str(m.group(2))
        if d1 and d2:
            return f"{d1} - {d2}"
    d = format_date_str(s)
    if d:
        return d
    return None

# ---------------------------- HTTP helpers ---------------------------------
def fetch_page(url, timeout=12):
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        if r.status_code == 200:
            return r.text
    except Exception as e:
        logd(f"fetch page error: {e}")
    return None

def try_ddgs_text(query, max_results=6):
    if not HAVE_DDGS:
        return []
    try:
        with DDGS() as dd:
            return list(dd.text(query, max_results=max_results))
    except Exception as e:
        logd(f"DDGS text error: {e}")
        return []

def ddgs_images(query, max_results=6):
    if not HAVE_DDGS:
        return []
    try:
        with DDGS() as dd:
            return [r.get("image") for r in dd.images(query, max_results=max_results) if r.get("image")]
    except Exception as e:
        logd(f"DDGS image error: {e}")
        return []

# ---------------------------- Image helpers --------------------------------
def download_image_to(url, path):
    try:
        r = requests.get(url, headers=HEADERS, timeout=12)
        if r.status_code == 200 and r.headers.get("content-type", "").startswith("image"):
            img = Image.open(BytesIO(r.content))
            img = img.convert("RGB")
            max_w, max_h = 600, 900
            img.thumbnail((max_w, max_h), Image.LANCZOS)
            os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            img.save(path, format="JPEG", quality=90)
            return True
    except Exception as e:
        logd(f"image download failed: {e}")
    return False

def build_absolute_url(local_path):
    local_path = local_path.replace("\\", "/")
    return GITHUB_PAGES_URL.rstrip("/") + "/" + local_path.lstrip("/")

def save_image_locally_from_url(img_url, show_id):
    """Save image found online into IMAGES_DIR with show_id as filename. Returns (local_path, remote_url)"""
    if not img_url:
        return None, None
    try:
        local_name = f"{show_id}.jpg" if show_id else safe_filename(img_url.split("/")[-1])
        local_path = os.path.join(IMAGES_DIR, local_name)
        ok = download_image_to(img_url, local_path)
        if ok:
            return local_path, build_absolute_url(local_path)
    except Exception as e:
        logd(f"save_image_locally_from_url error: {e}")
    return None, None

# ---------------------------- Parsing helpers from original -----------------
def clean_parenthesis_remove_cjk(s):
    if not s:
        return s
    return re.sub(r'\([^)]*[\u4e00-\u9fff\u3400-\u4dbf\uac00-\ud7af][^)]*\)', '', s)

def parse_synopsis_from_html(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    full_text = soup.get_text("\n", strip=True)
    syn_candidates = []
    meta = soup.find("meta", attrs={"name": "description"}) or soup.find("meta", attrs={"property": "og:description"})
    if meta and meta.get("content") and len(meta.get("content")) > 30:
        syn_candidates.append(meta.get("content").strip())
    for h in soup.find_all(re.compile("^h[1-6]$")):
        txt = h.get_text(" ", strip=True).lower()
        if any(k in txt for k in ("plot", "synopsis", "story", "summary")):
            parts = []
            for sib in h.find_next_siblings():
                if sib.name and re.match(r'^h[1-6]$', sib.name.lower()):
                    break
                if sib.name == 'p':
                    parts.append(sib.get_text(" ", strip=True))
                if sib.name in ('div', 'section'):
                    txt_inner = sib.get_text(" ", strip=True)
                    if txt_inner:
                        parts.append(txt_inner)
                if len(parts) >= 6:
                    break
            if parts:
                syn_candidates.append("\n\n".join(parts))
                break
    if not syn_candidates:
        for p in soup.find_all('p'):
            txt = p.get_text(" ", strip=True)
            if len(txt) > 80:
                syn_candidates.append(txt)
                break
    syn = syn_candidates[0] if syn_candidates else None
    duration = None
    try:
        lower = full_text.lower()
        m = re.search(r'(\b\d{2,3})\s*(?:min|minutes)\b', lower)
        if m:
            duration = int(m.group(1))
        else:
            m2 = re.search(r'runtime[^0-9]*(\d{1,3})', lower)
            if m2:
                duration = int(m2.group(1))
    except Exception:
        duration = None

    metadata = {}
    m = re.search(r'Also\s+Known\s+As[:\s]*([^\n\r]+)', full_text, flags=re.I)
    if m:
        other_raw = m.group(1).strip()
        metadata['otherNames'] = [p.strip() for p in re.split(r',\s*', other_raw) if p.strip()]
    else:
        metadata['otherNames'] = []

    m3 = re.search(r'(Release\s+Date|Aired|Aired on|Original release)[:\s]*([^\n\r]+)', full_text, flags=re.I)
    if m3:
        raw = m3.group(2).strip()
        rfmt = format_date_range(raw)
        if rfmt:
            metadata['releaseDateRaw'] = raw
            metadata['releaseDate'] = rfmt
        else:
            metadata['releaseDateRaw'] = raw
            metadata['releaseDate'] = raw
    else:
        m4 = re.search(r'([A-Za-z]+\s+\d{1,2},\s*\d{4})', full_text)
        if m4:
            metadata['releaseDateRaw'] = m4.group(1).strip()
            metadata['releaseDate'] = format_date_str(metadata['releaseDateRaw'])
        else:
            metadata['releaseDate'] = None

    if syn:
        syn = clean_parenthesis_remove_cjk(syn)
        paragraphs = [normalize_whitespace_and_sentences(p) for p in syn.split('\n\n') if p.strip()]
        syn = '\n\n'.join(paragraphs)
    domain = re.sub(r'^https?://(www\.)?', '', base_url).split('/')[0] if base_url else ''
    label = 'AsianWiki' if 'asianwiki' in domain else ('MyDramaList' if 'mydramalist' in domain else domain)
    syn_with_src = f"{syn} (Source: {label})" if syn else None
    return syn_with_src, duration, full_text, metadata

def pick_best_result(results):
    if not results:
        return None
    for r in results:
        url = r.get("href") or r.get("url") or r.get("link") or ""
        if any(site in url for site in ["mydramalist.com", "asianwiki.com", "wikipedia.org"]):
            return url
    return results[0].get("href") or results[0].get("url") or None

# ---------------------------- Grouped Fetch Blocks -------------------------
# Each of these groups contains 5 functions (synopsis, image, otherNames, duration, releaseDate)
# They use show_name + release_year to build queries/urls for higher accuracy.
# You can edit these blocks independently if a site changes layout.

# ============================================================
# 🏮 ASIANWIKI FETCHING BLOCKS
# ============================================================

def build_asianwiki_url(show_name, release_year=None):
    # AsianWiki pages often use underscored titles with year in parentheses for disambiguation.
    base = "https://asianwiki.com/"
    title = safe_filename(show_name).replace("_", " ").strip().replace(" ", "_")
    if release_year:
        # Try common pattern: Title_(YYYY)
        return base + f"{title}_({release_year})"
    return base + title

def fetch_synopsis_from_asianwiki(show_name, release_year):
    """Fetch synopsis from AsianWiki using show_name + release_year"""
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        # fallback to search via DDGS
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        return None, None, None, []
    syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
    return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', [])

def fetch_image_from_asianwiki(show_name, release_year, show_id):
    """Fetch image (poster) from AsianWiki page"""
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        # try ddgs image search
        imgs = ddgs_images(f"{show_name} {release_year} site:asianwiki.com")
        for img in imgs:
            local_path, remote = save_image_locally_from_url(img, show_id)
            if local_path:
                return local_path, remote, "asianwiki"
        return None, None, None
    soup = BeautifulSoup(html, "lxml")
    og = soup.find('meta', property='og:image')
    img_url = og.get('content') if og and og.get('content') else None
    if not img_url:
        img_tag = soup.find('img')
        img_url = img_tag.get('src') if img_tag and img_tag.get('src') else None
    if img_url and img_url.startswith('//'):
        img_url = 'https:' + img_url
    if img_url and img_url.startswith('/'):
        base = re.match(r'^(https?://[^/]+)', url)
        if base:
            img_url = base.group(1) + img_url
    if img_url:
        local_path, remote = save_image_locally_from_url(img_url, show_id)
        if local_path:
            return local_path, remote, "asianwiki"
    return None, None, None

def fetch_othernames_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        return []
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('otherNames', [])

def fetch_duration_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        return None
    _, dur, _, _ = parse_synopsis_from_html(html, url)
    return dur

def fetch_release_date_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        return None
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('releaseDate')

# ============================================================
# 🌏 MYDRAMALIST FETCHING BLOCKS
# ============================================================

def build_mydramalist_url(show_name, release_year=None):
    # MyDramaList uses URL slugs often like /show/<id>-<slug> but we attempt search queries
    # We will rely on DDGS search to find the best page when exact slug not known.
    query = f"{show_name} {release_year} site:mydramalist.com" if release_year else f"{show_name} site:mydramalist.com"
    results = try_ddgs_text(query)
    url = pick_best_result(results) if results else None
    return url

def fetch_synopsis_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None, None, None, []
    html = fetch_page(url)
    if not html:
        return None, None, None, []
    syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
    return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', [])

def fetch_image_from_mydramalist(show_name, release_year, show_id):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        # fallback to ddgs images
        imgs = ddgs_images(f"{show_name} {release_year} site:mydramalist.com")
        for img in imgs:
            local_path, remote = save_image_locally_from_url(img, show_id)
            if local_path:
                return local_path, remote, "mydramalist"
        return None, None, None
    html = fetch_page(url)
    if not html:
        return None, None, None
    soup = BeautifulSoup(html, "lxml")
    og = soup.find('meta', property='og:image')
    img_url = og.get('content') if og and og.get('content') else None
    if not img_url:
        img_tag = soup.find('img')
        img_url = img_tag.get('src') if img_tag and img_tag.get('src') else None
    if img_url and img_url.startswith('//'):
        img_url = 'https:'+img_url
    local_path, remote = save_image_locally_from_url(img_url, show_id)
    if local_path:
        return local_path, remote, "mydramalist"
    return None, None, None

def fetch_othernames_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return []
    html = fetch_page(url)
    if not html:
        return []
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('otherNames', [])

def fetch_duration_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None
    html = fetch_page(url)
    if not html:
        return None
    _, dur, _, _ = parse_synopsis_from_html(html, url)
    return dur

def fetch_release_date_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None
    html = fetch_page(url)
    if not html:
        return None
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('releaseDate')

# ---------------------------- Higher-level fetch orchestrators -------------
def fetch_synopsis_and_duration(show_name, release_year, prefer_sites=None, existing_synopsis=None, allow_replace=False, site_priority=None):
    """
    Orchestrates synopsis and duration fetching using preference order.
    Returns (synopsis, duration, releaseDate, otherNames, site_used_for_synopsis)
    """
    if existing_synopsis and not allow_replace:
        pass

    prefer_list = []
    if prefer_sites and HAVE_DDGS:
        unknowns = [p for p in prefer_sites if p and p.lower() not in ('asianwiki','mydramalist')]
        if unknowns:
            try:
                q = f"{show_name} {release_year or ''} {' '.join(unknowns)}"
                results = try_ddgs_text(q, max_results=6)
                url = pick_best_result(results) if results else None
                if url:
                    html = fetch_page(url)
                    if html:
                        syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
                        return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', []), 'ddgs_search'
            except Exception as e:
                logd(f"ddgs prefer_sites search failed: {e}")
    if site_priority and isinstance(site_priority, dict):
        primary = site_priority.get("synopsis")
        if primary:
            prefer_list.append(primary)
    if prefer_sites:
        for p in prefer_sites:
            if p not in prefer_list:
                prefer_list.append(p)
    for p in ("asianwiki", "mydramalist"):
        if p not in prefer_list:
            prefer_list.append(p)

    for site in prefer_list:
        try:
            if site == "asianwiki":
                syn, dur, rdate, other = fetch_synopsis_from_asianwiki(show_name, release_year)
            elif site == "mydramalist":
                syn, dur, rdate, other = fetch_synopsis_from_mydramalist(show_name, release_year)
            else:
                syn = dur = rdate = None
                other = []
            if syn:
                return syn, dur, rdate, other, site
            if rdate or other or dur:
                return syn, dur, rdate, other, site
        except Exception as e:
            logd(f"fetch_synopsis_and_duration site {site} failed: {e}")
    return (existing_synopsis or "Synopsis not available."), None, None, [], None

def fetch_and_save_image_for_show(show_name, prefer_sites, show_id, release_year=None, site_priority=None):
    """
    Orchestrates image fetching using site_priority preference. Returns (local_path, remote_url, site_used)
    """
    prefer_list = []
    if site_priority and isinstance(site_priority, dict):
        primary = site_priority.get("image")
        if primary:
            prefer_list.append(primary)
    if prefer_sites:
        for p in prefer_sites:
            if p not in prefer_list:
                prefer_list.append(p)
    for p in ("asianwiki", "mydramalist"):
        if p not in prefer_list:
            prefer_list.append(p)

    for site in prefer_list:
        try:
            if site == "asianwiki":
                lp, ru, used = fetch_image_from_asianwiki(show_name, release_year, show_id)
            elif site == "mydramalist":
                lp, ru, used = fetch_image_from_mydramalist(show_name, release_year, show_id)
            else:
                lp = ru = used = None
            if lp:
                return lp, ru, used
        except Exception as e:
            logd(f"fetch_and_save_image_for_show site {site} failed: {e}")
    try:
        imgs = ddgs_images(f"{show_name} {release_year if release_year else ''}")
        for img in imgs:
            if img:
                local_path, remote = save_image_locally_from_url(img, show_id)
                if local_path:
                    return local_path, remote, "ddgs"
    except Exception as e:
        logd(f"ddgs_images fallback failed: {e}")
    return None, None, None

# ---------------------------- Deletion processing (kept original logic) ---
def process_deletions(excel_file, json_file, report_changes):
    try:
        df = pd.read_excel(excel_file, sheet_name='Deleting Records')
    except Exception:
        return [], []
    if df.shape[1] < 1:
        return [], []
    cols = [str(c).strip().lower() for c in df.columns]
    id_col = None
    for i, c in enumerate(cols):
        if c == 'id' or 'id' in c:
            id_col = df.columns[i]
            break
    if id_col is None:
        id_col = df.columns[0]
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as jf:
                data = json.load(jf)
        except Exception:
            data = []
    else:
        data = []
    by_id = {int(o['showID']): o for o in data if 'showID' in o and isinstance(o['showID'], int)}
    to_delete = []
    for _, row in df.iterrows():
        val = row[id_col]
        if pd.isna(val):
            continue
        try:
            to_delete.append(int(val))
        except Exception:
            continue
    if not to_delete:
        return [], []
    os.makedirs(DELETED_DATA_DIR, exist_ok=True)
    deleted_ids = []
    not_found_ids = []
    for iid in to_delete:
        if iid in by_id:
            deleted_obj = by_id.pop(iid)
            deleted_ids.append(iid)
            fname = f"DELETED_{now_ist().strftime('%d_%B_%Y_%H%M')}_{iid}.json"
            outpath = os.path.join(DELETED_DATA_DIR, safe_filename(fname))
            try:
                with open(outpath, 'w', encoding='utf-8') as of:
                    json.dump(deleted_obj, of, indent=4, ensure_ascii=False)
                report_changes.setdefault('deleted', []).append(f"{iid} -> {deleted_obj.get('showName', 'Unknown')} ({deleted_obj.get('releasedYear', 'N/A')}) -> ✅ Deleted and archived -> {outpath}")
                try:
                    img_url = deleted_obj.get('showImage') or ""
                    if img_url:
                        candidate = None
                        m = re.search(r'/(images/[^/?#]+)$', img_url)
                        if m:
                            candidate = m.group(1)
                        elif img_url.startswith('images/'):
                            candidate = img_url
                        if candidate:
                            src = os.path.join('.', candidate)
                            if os.path.exists(src):
                                os.makedirs(DELETE_IMAGES_DIR, exist_ok=True)
                                dst_name = f"{iid}_{filename_timestamp()}.jpg"
                                dst = os.path.join(DELETE_IMAGES_DIR, safe_filename(dst_name))
                                shutil.move(src, dst)
                                report_changes.setdefault('deleted_images_moved', []).append(f"{iid} -> {deleted_obj.get('showName', 'Unknown')} ({deleted_obj.get('releasedYear', 'N/A')}) -> image moved: {src} -> {dst}")
                except Exception as e_img:
                    report_changes.setdefault('deleted_images_moved', []).append(f"{iid} -> ⚠️ Image move failed: {e_img}")
            except Exception as e:
                report_changes.setdefault('deleted', []).append(f"{iid} -> ⚠️ Deletion recorded but failed to write archive: {e}")
        else:
            not_found_ids.append(iid)
            report_changes.setdefault('deleted_not_found', []).append(f"-{iid} -> ❌ Not found in seriesData.json")
    merged = sorted(by_id.values(), key=lambda x: x.get('showID', 0))
    try:
        with open(json_file, 'w', encoding='utf-8') as jf:
            json.dump(merged, jf, indent=4, ensure_ascii=False)
        report_changes.setdefault('deleted_summary', []).append(
            f"seriesData.json updated after deletions (deleted {len(deleted_ids)} items)."
        )
    except Exception as e:
        report_changes.setdefault('deleted_summary', []).append(f"⚠️ Failed to write updated {json_file}: {e}")
    return deleted_ids, not_found_ids

# ---------------------------- Manual Updates (Refactored) ----------
def apply_manual_updates(excel_file: str, by_id: dict):
    """
    Applies updates from the 'Manual Update' sheet directly to the in-memory
    dictionary of objects. Returns a list of changes for reporting.
    """
    sheet = 'Manual Update'
    try:
        df = pd.read_excel(excel_file, sheet_name=sheet)
    except Exception:
        print("ℹ️ No 'Manual Update' sheet found; skipping Manual Updates.")
        return []
    if df.shape[1] < 2:
        print("Manual Update sheet must have at least two columns: showID and dataString")
        return []

    reportable_changes = []
    fetched_fields_map = {
        'synopsis': 'synopsis', 'showimage': 'image', 'image': 'image',
        'duration': 'Duration', 'othernames': 'otherNames', 'releasedate': 'releaseDate'
    }
    for _, row in df.iterrows():
        sid = None
        try:
            sid = int(row[0]) if not pd.isna(row[0]) else None
        except Exception:
            continue
        if sid is None or sid not in by_id:
            continue

        raw = row[1]
        if pd.isna(raw) or not str(raw).strip():
            continue
        s = str(raw).strip()
        try:
            upd = json.loads(s) if s.startswith('{') else {k.strip(): v.strip() for k, v in (p.split(':', 1) for p in s.split(',') if ':' in p)}
        except Exception:
            continue
        if not upd:
            continue

        obj = by_id[sid]
        old_obj = copy.deepcopy(obj)
        changed_fields = []

        for k, v in upd.items():
            # Normalize keys for comparison
            key_for_obj = k
            if k.lower() == 'ratings': key_for_obj = 'ratings'
            if k.lower() in ('releasedyear', 'year'): key_for_obj = 'releasedYear'
            
            # Type casting for specific fields
            new_val = v
            if key_for_obj in ('ratings', 'releasedYear', 'totalEpisodes', 'Duration'):
                try: new_val = int(v)
                except (ValueError, TypeError): continue
            
            if obj.get(key_for_obj) != new_val:
                obj[key_for_obj] = new_val
                changed_fields.append(key_for_obj)
                # If a fetched field is manually updated, mark it in sitePriorityUsed
                if k.lower() in fetched_fields_map:
                    spu = obj.setdefault('sitePriorityUsed', {})
                    spu[fetched_fields_map[k.lower()]] = 'Manual'
        
        if changed_fields:
            human_readable_changes = [human_readable_field(f) for f in changed_fields]
            obj['updatedOn'] = now_ist().strftime('%d %B %Y')
            obj['updatedDetails'] = f"{', '.join(human_readable_changes)} Updated Manually By Owner"
            reportable_changes.append({"old": old_obj, "new": obj})

    if reportable_changes:
        print(f"✅ Identified {len(reportable_changes)} Manual Updates to apply.")
    else:
        print("ℹ️ No valid Manual Updates found/applied.")
        
    return reportable_changes


# ---------------------------- Excel -> objects mapping ----------------------
COLUMN_MAP = {
    "no": "showID", "series title": "showName", "started date": "watchStartedOn", "finished date": "watchEndedOn",
    "year": "releasedYear", "total episodes": "totalEpisodes", "original language": "nativeLanguage", "language": "watchedLanguage",
    "ratings": "ratings", "catagory": "genres", "category": "genres", "original network": "network", "comments": "comments"
}

def tidy_comment(val):
    if pd.isna(val) or not str(val).strip():
        return None
    text = re.sub(r'\s+', ' ', str(val)).strip()
    if not text.endswith('.'):
        text = text + '.'
    text = re.sub(r'\.([^\s])', r'. \1', text)
    return text

def sheet_base_offset(sheet_name: str) -> int:
    if sheet_name == "Sheet1":
        return 100
    if sheet_name == "Feb 7 2023 Onwards":
        return 1000
    if sheet_name == "Sheet2":
        return 3000
    return 0

# ---------------------------- Helper: save metadata backup ------------------
def save_metadata_backup(show_id, show_name, language, fetched_fields, site_priority_used):
    try:
        os.makedirs(BACKUP_META_DIR, exist_ok=True)
        fname = f"META_{now_ist().strftime('%d_%B_%Y_%H%M')}_{show_id}.json"
        outpath = os.path.join(BACKUP_META_DIR, safe_filename(fname))
        payload = {
            "scriptVersion": SCRIPT_VERSION,
            "showID": show_id,
            "showName": show_name,
            "language": language,
            "timestamp": now_ist().strftime("%d %B %Y %I:%M %p (IST)"),
            "fetchedFields": fetched_fields,
            "sitePriorityUsed": site_priority_used
        }
        with open(outpath, 'w', encoding='utf-8') as of:
            json.dump(payload, of, indent=2, ensure_ascii=False)
        return outpath
    except Exception as e:
        logd(f"save_metadata_backup failed: {e}")
        return None

# ---------------------------- Cleanup Functions ---------------------
def cleanup_deleted_data(retention_days=None):
    """Removes old archived JSON files from the deleted-data directory."""
    if retention_days is None:
        retention_days = KEEP_OLD_IMAGES_DAYS # Reuse the same retention period for simplicity
    if not os.path.exists(DELETED_DATA_DIR):
        return 0
    cutoff = datetime.now() - timedelta(days=retention_days)
    removed = 0
    for fname in os.listdir(DELETED_DATA_DIR):
        path = os.path.join(DELETED_DATA_DIR, fname)
        try:
            mtime = datetime.fromtimestamp(os.path.getmtime(path))
            if mtime < cutoff:
                os.remove(path)
                removed += 1
                print(f"🧹 Removed expired deleted data archive: {path}")
        except Exception as e:
            print(f"⚠️ Could not cleanup deleted data archive {path}: {e}")
    return removed

def cleanup_old_metadata_backups(retention_days=METADATA_BACKUP_RETENTION_DAYS):
    if not os.path.exists(BACKUP_META_DIR):
        return 0
    cutoff = datetime.now() - timedelta(days=retention_days)
    removed = 0
    for fname in os.listdir(BACKUP_META_DIR):
        path = os.path.join(BACKUP_META_DIR, fname)
        try:
            mtime = datetime.fromtimestamp(os.path.getmtime(path))
            if mtime < cutoff:
                os.remove(path)
                removed += 1
                print(f"🧹 Removed expired metadata backup: {path}")
        except Exception as e:
            print(f"⚠️ Could not cleanup metadata backup {path}: {e}")
    return removed

# ---------------------------- Excel to objects (with field locking) -
def excel_to_objects(excel_file, sheet_name, existing_by_id, report_changes, start_index=0, max_items=None, time_limit_seconds=None,
                     deleted_ids_for_run=None, deleting_not_found_initial=None, deleting_found_in_sheets=None):
    df = pd.read_excel(excel_file, sheet_name=sheet_name)
    df.columns = [c.strip().lower() for c in df.columns]
    again_idx = None
    for i, c in enumerate(df.columns):
        if "again watched" in c:
            again_idx = i
            break
    if again_idx is None:
        raise ValueError(f"'Again Watched Date' columns not found in sheet: {sheet_name}")
    items = []
    processed = 0
    start_time = time.time()
    last_idx = start_index
    total_rows = len(df)
    for idx in range(start_index, total_rows):
        if max_items and processed >= max_items:
            break
        if time_limit_seconds and (time.time() - start_time) >= time_limit_seconds:
            break
        row = df.iloc[idx]
        obj = {}
        try:
            for col in df.columns[:again_idx]:
                key = COLUMN_MAP.get(col, col)
                val = row[col]
                if key == "showID":
                    base = sheet_base_offset(sheet_name)
                    obj["showID"] = base + int(val) if pd.notna(val) else None
                elif key == "showName":
                    raw_name = str(val) if pd.notna(val) else ""
                    clean_name = re.sub(r'\s+', ' ', raw_name).strip()
                    obj["showName"] = clean_name if clean_name else None
                elif key in ("watchStartedOn", "watchEndedOn"):
                    obj[key] = ddmmyyyy(val)
                elif key == "releasedYear":
                    obj[key] = int(val) if pd.notna(val) else None
                elif key == "totalEpisodes":
                    obj[key] = int(val) if pd.notna(val) else None
                elif key == "nativeLanguage":
                    obj[key] = str(val).strip().capitalize() if pd.notna(val) else None
                elif key == "watchedLanguage":
                    obj[key] = str(val).strip().capitalize() if pd.notna(val) else None
                elif key == "comments":
                    obj[key] = tidy_comment(val)
                elif key == "ratings":
                    try:
                        obj[key] = int(val) if pd.notna(val) else 0
                    except Exception:
                        obj[key] = 0
                elif key == "genres":
                    obj[key] = normalize_list_from_csv(val, cap=False, strip=True)
                elif key == "network":
                    obj[key] = normalize_list_from_csv(val, cap=False, strip=True)
                else:
                    obj[key] = str(val).strip() if pd.notna(val) else None
            obj["showType"] = "Mini Drama" if sheet_name.lower() == "mini drama" else "Drama"
            obj["country"] = None
            native = obj.get("nativeLanguage")
            if native:
                n = str(native).strip().lower()
                if n in ("korean", "korea", "korean language"): obj["country"] = "South Korea"
                elif n in ("chinese", "china", "mandarin"): obj["country"] = "China"
                elif n in ("japanese", "japan"): obj["country"] = "Japan"
            
            dates = [ddmmyyyy(v) for v in row[again_idx:] if ddmmyyyy(v)]
            obj["againWatchedDates"] = dates

            sid = obj.get("showID")
            if not sid: continue

            if deleted_ids_for_run and sid in deleted_ids_for_run:
                report_changes.setdefault('ignored_deleting', []).append(f'{sid} -> Already Deleted')
                if deleting_not_found_initial and sid in deleting_not_found_initial:
                    deleting_found_in_sheets.add(sid)
                continue
            if deleting_not_found_initial and sid in deleting_not_found_initial:
                deleting_found_in_sheets.add(sid)

            show_name = obj.get("showName")
            released_year = obj.get("releasedYear")
            language = (obj.get("nativeLanguage") or "").strip().lower()
            site_priority = SITE_PRIORITY_BY_LANGUAGE.get(language, SITE_PRIORITY_BY_LANGUAGE.get("default"))

            existing = existing_by_id.get(sid)

            if existing is None:
                # --- NEW OBJECT: Fetch data and set initial metadata ---
                obj["updatedOn"] = now_ist().strftime("%d %B %Y")
                obj["updatedDetails"] = "First Time Uploaded"
                r = int(obj.get("ratings") or 0)
                obj["topRatings"] = r * (len(dates) if len(dates) > 0 else 1) * 100
                
                metadata_backup_fields = {}
                site_priority_used = {}
                
                # Fetch image
                try:
                    local_path, remote_url, img_site = fetch_and_save_image_for_show(show_name, None, sid, released_year, site_priority)
                    if local_path:
                        obj["showImage"] = build_absolute_url(local_path)
                        report_changes.setdefault('images', []).append({'showID': sid, 'showName': show_name, 'new': obj["showImage"]})
                        metadata_backup_fields["showImage"] = {"value": obj["showImage"], "source": img_site}
                        site_priority_used["image"] = img_site or site_priority.get("image")
                except Exception as e:
                    logd(f"Image fetch failed for {show_name}: {e}")

                # Fetch synopsis, duration, etc.
                try:
                    syn, dur, rdate, other, site = fetch_synopsis_and_duration(show_name, released_year, site_priority=site_priority)
                    if syn: obj["synopsis"] = normalize_whitespace_and_sentences(syn)
                    if rdate: obj['releaseDate'] = format_date_range(rdate) or format_date_str(rdate) or rdate
                    if other: obj['otherNames'] = other
                    if dur: obj['Duration'] = int(dur)

                    if site:
                        if syn: metadata_backup_fields["synopsis"] = {"value": syn, "source": site}; site_priority_used["synopsis"] = site
                        if rdate: metadata_backup_fields["releaseDate"] = {"value": rdate, "source": site}; site_priority_used["releaseDate"] = site
                        if other: metadata_backup_fields["otherNames"] = {"value": other, "source": site}; site_priority_used["otherNames"] = site
                        if dur: metadata_backup_fields["Duration"] = {"value": dur, "source": site}; site_priority_used["duration"] = site
                except Exception as e:
                    logd(f"Metadata fetch failed for {show_name}: {e}")

                if metadata_backup_fields:
                    backup_path = save_metadata_backup(sid, show_name, language, metadata_backup_fields, site_priority_used)
                    if backup_path:
                        report_changes.setdefault('metadata_backups_created', []).append(backup_path)
                
                if site_priority_used:
                    obj['sitePriorityUsed'] = site_priority_used
                    obj['sourceSites'] = site_priority_used

            else:
                # --- EXISTING OBJECT: Preserve locked fields ---
                for field in LOCKED_FIELDS_AFTER_CREATION:
                    if field in existing:
                        obj[field] = existing[field]
                
                # Recalculate topRatings based on current ratings and againWatchedDates from Excel
                r = int(obj.get("ratings") or 0)
                obj["topRatings"] = r * (len(obj.get("againWatchedDates", [])) + 1) * 100

                # Special case: if image was missing, try to fetch it once
                if not obj.get("showImage"):
                    try:
                        local_path, remote_url, img_site = fetch_and_save_image_for_show(show_name, None, sid, released_year, site_priority)
                        if local_path:
                            obj["showImage"] = build_absolute_url(local_path)
                            report_changes.setdefault('images', []).append({'showID': sid, 'showName': show_name, 'new': obj["showImage"]})
                    except Exception as e:
                        logd(f"Existing object image fetch failed for {show_name}: {e}")

            # Ensure all keys are present for consistency
            final_obj = {
                "showID": obj.get("showID"), "showName": obj.get("showName"),
                "otherNames": obj.get("otherNames", []), "showImage": obj.get("showImage"),
                "watchStartedOn": obj.get("watchStartedOn"), "watchEndedOn": obj.get("watchEndedOn"),
                "releasedYear": obj.get("releasedYear"), "releaseDate": obj.get("releaseDate"),
                "totalEpisodes": obj.get("totalEpisodes"), "showType": obj.get("showType"),
                "nativeLanguage": obj.get("nativeLanguage"), "watchedLanguage": obj.get("watchedLanguage"),
                "country": obj.get("country"), "comments": obj.get("comments"), "ratings": obj.get("ratings"),
                "genres": obj.get("genres", []), "network": obj.get("network", []),
                "againWatchedDates": obj.get("againWatchedDates", []), "updatedOn": obj.get("updatedOn"),
                "updatedDetails": obj.get("updatedDetails"), "synopsis": obj.get("synopsis"),
                "topRatings": obj.get("topRatings"), "Duration": obj.get("Duration"),
                "sitePriorityUsed": obj.get("sitePriorityUsed", {}), "sourceSites": obj.get("sourceSites", {})
            }
            items.append(final_obj)
            processed += 1
            last_idx = idx
        except Exception as e:
            raise RuntimeError(f"Row {idx+2} in sheet '{sheet_name}' processing failed: {e}\n{traceback.format_exc()}")
            
    finished = (last_idx >= total_rows - 1) if total_rows > 0 else True
    next_index = (last_idx + 1) if processed > 0 else start_index
    return items, processed, finished, next_index

# ---------------------------- Reports --------------------------------------

def write_report(report_changes_by_sheet, report_path, start_time=None, end_time=None, metadata_backups_removed=0):
    lines = []
    lines.append("✅ Workflow completed successfully")
    end_time = end_time or now_ist()
    start_time = start_time or end_time
    lines.append(f"📅 Run Time: {end_time.strftime('%d %B %Y %I:%M %p (IST)')}")
    duration_td = end_time - start_time
    mins, secs = divmod(int(duration_td.total_seconds()), 60)
    lines.append(f"🕒 Duration: {mins} min {secs} sec")
    lines.append(f"⚙️ Script Version: {SCRIPT_VERSION}")
    lines.append("")
    sep = "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    total_created = total_updated = total_deleted = total_skipped = 0
    grand_rows = 0

    for sheet, changes in report_changes_by_sheet.items():
        if not changes: continue

        lines.append(sep)
        lines.append(f"🗂️ === {sheet} Summary ===")
        lines.append(sep)
        
        created = changes.get('created', [])
        if created:
            lines.append("\n🆕 Data Created:")
            for obj in created:
                lines.append(f"- {obj.get('showID','N/A')} - {obj.get('showName','Unknown')} ({obj.get('releasedYear','N/A')}) -> First Time Uploaded")
        
        updated = changes.get('updated', [])
        if updated:
            lines.append("\n✨ Data Updated:")
            for pair in updated:
                new = pair.get('new', {})
                details = new.get('updatedDetails', 'Updated')
                emoji = "✍️" if "Manually By Owner" in details else "🔁"
                lines.append(f"{emoji} {new.get('showID','N/A')} - {new.get('showName','Unknown')} ({new.get('releasedYear','N/A')}) -> {details}")

        skipped = changes.get('skipped', [])
        if skipped:
            lines.append("\n🚫 Unchanged Entries (Skipped):")
            for name in skipped:
                lines.append(f"- {name}")
        
        images = changes.get('images', [])
        if images:
            lines.append("\n🖼️ Image Updated:")
            for itm in images:
                lines.append(f"- {itm.get('showID','N/A')} - {itm.get('showName','Unknown')} -> New image saved")

        deleted = changes.get('deleted', [])
        if deleted:
            lines.append("\n❌ Data Deleted:")
            for d in deleted:
                lines.append(f"- {d.split('-> ✅')[0].strip()}")
        
        if changes.get('fetch_errors'):
            lines.append("\n⚠️ Fetch Errors:")
            for fe in changes.get('fetch_errors'):
                lines.append(f"- {fe}")
        
        lines.append("")
        lines.append(sep)
        ccount = len(created)
        ucount = len(updated)
        scount = len(skipped)
        dcount = len(deleted)
        total_created += ccount
        total_updated += ucount
        total_skipped += scount
        total_deleted += dcount
        rows_count = changes.get('rows_processed', ccount + ucount + scount)
        if sheet not in ('Deleting Records', 'Manual Updates'):
            grand_rows += rows_count

        lines.append(f"📊 Stats (Sheet: {sheet})")
        lines.append(f"  - Created: {ccount}, Updated: {ucount}, Skipped: {scount}, Deleted: {dcount}")
        lines.append(f"  - Total Rows Processed: {rows_count}")
        lines.append("")

    lines.append(sep)
    lines.append("📊 Overall Summary")
    lines.append(sep)
    lines.append(f"🆕 Total Created: {total_created}")
    lines.append(f"✨ Total Updated: {total_updated}")
    lines.append(f"🚫 Total Skipped: {total_skipped}")
    lines.append(f"❌ Total Deleted: {total_deleted}")
    lines.append(f"💾 Backup files: {len(os.listdir(BACKUP_DIR)) if os.path.exists(BACKUP_DIR) else 0}")
    lines.append(f"  Grand Total Rows Processed: {grand_rows}")

    lines.append("")
    lines.append(f"💾 Metadata Backups Created: {sum(len(ch.get('metadata_backups_created', [])) for ch in report_changes_by_sheet.values())}")
    lines.append(f"🧹 Cleaned up old metadata backups: {metadata_backups_removed} removed (older than {METADATA_BACKUP_RETENTION_DAYS} days)")
    lines.append("")

    try:
        if os.path.exists(JSON_FILE):
            with open(JSON_FILE, 'r', encoding='utf-8') as jf:
                arr = json.load(jf)
                lines.append(f"📦 Total Objects in seriesData.json: {len(arr)}")
    except Exception:
        lines.append("📦 Total Objects in seriesData.json: Unknown (could not read file)")
    
    lines.append("\n🏁 Workflow finished successfully")
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))
    except Exception as e:
        print(f"⚠️ Could not write TXT report: {e}")


# ---------------------------- Secret scan & email body ---------------------
def scan_for_possible_secrets():
    findings = []
    if os.path.exists(SERVICE_ACCOUNT_FILE):
        try:
            with open(SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8') as f:
                s = f.read()
                has_private_key = 'private_key' in s
                m = re.search(r'"client_email"\s*:\s*"([^"]+)"', s)
                client_email = m.group(1) if m else None
                findings.append({'file': SERVICE_ACCOUNT_FILE, 'present': True, 'client_email': client_email, 'has_private_key': has_private_key, 'note': 'Service account JSON detected'})
        except Exception as e:
            findings.append({'file': SERVICE_ACCOUNT_FILE, 'present': True, 'note': f'Could not read file safely: {e}'})
    if os.path.exists(EXCEL_FILE_ID_TXT):
        try:
            s = open(EXCEL_FILE_ID_TXT, 'r', encoding='utf-8').read().strip()
            findings.append({'file': EXCEL_FILE_ID_TXT, 'present': True, 'length': len(s), 'note': 'Excel file id present (not shown)'})
        except Exception as e:
            findings.append({'file': EXCEL_FILE_ID_TXT, 'present': True, 'note': f'Could not read file: {e}'})
    
    return findings

def compose_email_body_from_report(report_path):
    body_lines = []
    body_lines.append("SECRETS CHECK:")
    findings = scan_for_possible_secrets()
    if not findings:
        body_lines.append("- No obvious secret files detected in the workspace.")
    else:
        for f in findings:
            line = f"- File: {f.get('file')} — note: {f.get('note')}."
            body_lines.append(line)
        body_lines.append("\nIf any of the above files were accidentally committed, rotate keys immediately.")
    body_lines.append("\n--- REPORT CONTENT (pasted below) ---\n")
    body_lines.append(f"Run Report — {now_ist().strftime('%d %B %Y %H:%M')}")
    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            body_lines.append(f.read())
    except Exception as e:
        body_lines.append(f"⚠️ Could not read report file for email body: {e}")
    return "\n".join(body_lines)

# ---------------------------- Excel fetch from GDrive (preserve original) -
def fetch_excel_from_gdrive_bytes(excel_file_id, service_account_path):
    if not HAVE_GOOGLE_API:
        print("ℹ️ Google API packages not available. Install with: pip install google-api-python-client google-auth-httplib2 google-auth")
        return None
    try:
        scopes = ['https://www.googleapis.com/auth/drive.readonly']
        creds = service_account.Credentials.from_service_account_file(service_account_path, scopes=scopes)
        drive_service = build('drive', 'v3', credentials=creds, cache_discovery=False)
        
        # Try direct download first (for .xlsx files)
        try:
            request = drive_service.files().get_media(fileId=excel_file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
            fh.seek(0)
            return fh
        except Exception:
            # Fallback to export (for Google Sheets)
            export_mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            request = drive_service.files().export_media(fileId=excel_file_id, mimeType=export_mime)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
            fh.seek(0)
            return fh
    except Exception as e:
        logd(f"Google Drive fetch failed: {e}\n{traceback.format_exc()}")
        return None

# ---------------------------- Main updater (reordered & integrated) -----
def update_json_from_excel(excel_file_like, json_file, sheet_names, max_per_run=0, max_run_time_minutes=0):
    print(f"🚀 Running create_update_backup_delete.py — Version {SCRIPT_VERSION}")
    start_time = now_ist()
    
    # Load existing JSON
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                old_objects = json.load(f)
        except Exception: old_objects = []
    else: old_objects = []
    
    old_by_id = {o['showID']: o for o in old_objects if 'showID' in o}
    report_changes_by_sheet = {}

    # 1. Process deletions first, which modifies the JSON on disk
    excel_file_like.seek(0)
    del_report = {}
    deleted_ids, deleting_not_found_initial = process_deletions(excel_file_like, json_file, del_report)
    if del_report: report_changes_by_sheet['Deleting Records'] = del_report

    # 2. Reload JSON after deletions and apply manual updates in memory
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                current_objects = json.load(f)
        except Exception: current_objects = []
    else: current_objects = []
    
    merged_by_id = {o['showID']: o for o in current_objects if 'showID' in o}
    
    excel_file_like.seek(0)
    manual_updates = apply_manual_updates(excel_file_like, merged_by_id)
    if manual_updates:
        report_changes_by_sheet['Manual Updates'] = {'updated': manual_updates}

    # 3. Process main sheets against the now-updated in-memory data
    progress = load_progress()
    time_limit_seconds = max_run_time_minutes * 60 if max_run_time_minutes > 0 else None
    processed_total = 0
    overall_continued = False
    deleting_found_in_sheets = set()

    for s in sheet_names:
        excel_file_like.seek(0)
        report_changes = {}
        start_idx = int(progress.get(s, 0))
        
        items, processed, finished, next_start_idx = excel_to_objects(excel_file_like, s, merged_by_id, report_changes,
                                                                      start_index=start_idx, max_items=max_per_run or None,
                                                                      time_limit_seconds=time_limit_seconds, deleted_ids_for_run=set(deleted_ids),
                                                                      deleting_not_found_initial=set(deleting_not_found_initial),
                                                                      deleting_found_in_sheets=deleting_found_in_sheets)
        
        modified_for_backup = []
        for new_obj in items:
            sid = new_obj.get('showID')
            old_obj = merged_by_id.get(sid)

            if old_obj is None:
                merged_by_id[sid] = new_obj
                report_changes.setdefault('created', []).append(new_obj)
            else:
                if objects_differ(old_obj, new_obj, ignore_keys=LOCKED_FIELDS_AFTER_CREATION):
                    # Generate detailed update description
                    change_descs = []
                    simple_fields = {'ratings', 'releasedYear', 'totalEpisodes'}
                    for k, v in new_obj.items():
                        if k not in old_obj or v != old_obj[k] and k not in LOCKED_FIELDS_AFTER_CREATION:
                            if k in simple_fields:
                                change_descs.append(f"{human_readable_field(k)}: {old_obj.get(k)} -> {v}")
                            else:
                                change_descs.append(f"{human_readable_field(k)} Updated")
                    
                    new_obj['updatedOn'] = now_ist().strftime('%d %B %Y')
                    new_obj['updatedDetails'] = ", ".join(change_descs) if change_descs else "Record Updated"
                    
                    report_changes.setdefault('updated', []).append({'old': old_obj, 'new': new_obj})
                    merged_by_id[sid] = new_obj
                    modified_for_backup.append(new_obj)
                else:
                    report_changes.setdefault('skipped', []).append(f"{sid} - {old_obj.get('showName', 'Unknown')}")
        
        if modified_for_backup:
            os.makedirs(BACKUP_DIR, exist_ok=True)
            backup_name = os.path.join(BACKUP_DIR, f"updated_{filename_timestamp()}_{safe_filename(s)}.json")
            with open(backup_name, 'w', encoding='utf-8') as bf:
                json.dump(modified_for_backup, bf, indent=4, ensure_ascii=False)
            print(f"✅ Backup of modified items saved -> {backup_name}")

        report_changes['rows_processed'] = processed
        if report_changes:
            report_changes_by_sheet[s] = report_changes

        processed_total += processed
        if not finished:
            progress[s] = next_start_idx
            overall_continued = True
        elif s in progress:
            del progress[s]
        save_progress(progress)

    # 4. Finalize and write files
    merged = sorted(merged_by_id.values(), key=lambda x: x.get('showID', 0))
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(merged, f, indent=4, ensure_ascii=False)

    os.makedirs(REPORTS_DIR, exist_ok=True)
    report_path = os.path.join(REPORTS_DIR, f"report_{filename_timestamp()}.txt")
    
    # Cleanup old files
    if SCHEDULED_RUN:
        cleanup_deleted_data()
    if os.path.exists(DELETE_IMAGES_DIR):
        cutoff = datetime.now() - timedelta(days=KEEP_OLD_IMAGES_DAYS)
        for fname in os.listdir(DELETE_IMAGES_DIR):
            path = os.path.join(DELETE_IMAGES_DIR, fname)
            try:
                if datetime.fromtimestamp(os.path.getmtime(path)) < cutoff:
                    os.remove(path)
            except Exception as e:
                print(f"⚠️ Could not cleanup old image {path}: {e}")
                
    removed_meta = cleanup_old_metadata_backups()
    
    # Write report and email body
    write_report(report_changes_by_sheet, report_path, start_time, now_ist(), removed_meta)
    print(f"✅ Report written -> {report_path}")
    
    email_body = compose_email_body_from_report(report_path)
    print('\n===EMAIL_BODY_START===\n')
    print(email_body)
    print('\n===EMAIL_BODY_END===\n')
    
    with open(STATUS_JSON, 'w', encoding='utf-8') as sf:
        json.dump({"continued": overall_continued, "processed_total": processed_total}, sf, indent=2)

    return

# ---------------------------- Entrypoint -----------------------------------
if __name__ == '__main__':
    if not (os.path.exists(EXCEL_FILE_ID_TXT) and os.path.exists(SERVICE_ACCOUNT_FILE)):
        print("❌ Missing GDrive credentials. Ensure EXCEL_FILE_ID.txt and GDRIVE_SERVICE_ACCOUNT.json are present.")
        sys.exit(3)
        
    try:
        with open(EXCEL_FILE_ID_TXT, 'r', encoding='utf-8') as f:
            excel_id = f.read().strip()
        if not excel_id:
            print("❌ EXCEL_FILE_ID.txt is empty. Aborting.")
            sys.exit(0)
    except Exception:
        print("❌ Could not read EXCEL_FILE_ID.txt. Aborting.")
        sys.exit(1)

    _sheets_env = os.environ.get("SHEETS", "Sheet1").strip()
    SHEETS = [s.strip() for s in _sheets_env.split(";") if s.strip()]

    excel_bytes = fetch_excel_from_gdrive_bytes(excel_id, SERVICE_ACCOUNT_FILE)
    if excel_bytes is None:
        print("❌ Could not fetch Excel file from Google Drive. Exiting.")
        sys.exit(1)
    
    try:
        update_json_from_excel(excel_bytes, JSON_FILE, SHEETS, max_per_run=MAX_PER_RUN, max_run_time_minutes=MAX_RUN_TIME_MINUTES)
    except Exception as e:
        print(f"❌ An unexpected error occurred: {e}")
        logd(traceback.format_exc())
        sys.exit(1)
        
    print("All done.")