# ============================================================
# Script: create_update_backup_delete.py
# Author: [BruceBanner001]
# Description:
#   This script automates the creation, update, and backup process
#   for JSON data objects derived from Excel or YAML workflows.
#
#   Key features:
#   - One backup per workflow run (contains only modified objects).
#   - Intelligent field merging (preserves 'otherNames', etc. when incoming empty).
#   - Skipped detection for unchanged records.
#   - Detailed reporting with per-field change summaries.
#   - Clean, scalable structure with clear comments.
#
# ============================================================


# ============================================================================
# Patched Script: create_update_backup_delete.py
# Purpose: Excel -> JSON automation (patched for enhanced synopsis/image fetching,
#          deletion handling, and single-email composing for CI workflows).
#
# IMPORTANT NOTES FOR MAINTENANCE / EXTENSION
# -------------------------------------------------
# 1) Site Preferences / Language mapping:
#    - The code uses a simple mapping to decide "preferred sites" for fetching synopsis
#      and images based on the show's native language (nativeLanguage field).
#    - To add a new language preference:
#        a) Locate the function excel_to_objects(...) and find the block that sets `prefer`.
#        b) Add a new branch for the language name (use lowercased checks, and include
#           possible variants, e.g. 'korean', 'korea', 'korean language').
#        c) Update fetch_synopsis_and_duration(...) and fetch_and_save_image_for_show(...)
#           if you want to treat the new site specially (site-specific parsing).
#
# 2) Adding a new preferred site parser:
#    - If you want more accurate extraction from a particular site (e.g., 'asianwiki' or 'mydramalist'),
#      add a new branch in parse_synopsis_from_html(...) that checks the domain and applies
#      site-specific DOM selectors (e.g., look for elements with id/class 'synopsis', 'summary',
#      or meta property 'og:description'). Keep a generic fallback for robustness.
#
# 3) Email behavior for CI (GitHub Actions):
#    - This script now composes the full email body in-memory via compose_email_body_from_report(report_path).
#      We purposely **do not** write email_body_*.txt files to disk anymore.
#    - The workflow should either:
#        A) read reports/report_*.txt and send email with that content, or
#        B) run this script and capture the printed email body (stdout) and pass it to the email action.
#    - The subject format required by the owner: "[Manual] Workflow <DD Month YYYY HHMM> Report"
#
# 4) Deletion behavior:
#    - When a showID is deleted via the "Deleting Records" sheet:
#        * the deleted object is saved to deleted-data/DELETED_<timestamp>_<id>.json
#        * the associated image file (if present under images/) is moved to old-images/
#        * a report entry is generated for moved images: 'deleted_images_moved'
#
# 5) Synopsis length:
#    - Controlled by SYNOPSIS_MAX_LEN environment variable (default 1500). Soft truncation attempts to cut at sentence end.
#
# 6) Debugging:
#    - Set DEBUG_FETCH=true in env to print useful debug messages.
#
# ============================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Script: create_update_backup_delete.py
# Purpose: Excel -> JSON automation (patched to not require a local Excel file).
#
# Requirements (recommended):
#   pip install pandas requests beautifulsoup4 pillow openpyxl google-api-python-client google-auth-httplib2 google-auth
# Notes:
#   - The script expects two files to be present in the runner environment:
#       EXCEL_FILE_ID.txt         (text file containing the Google Drive file id)
#       GDRIVE_SERVICE_ACCOUNT.json  (service account JSON key)
#   - If google-api-python-client/google-auth packages are not available, script
#     will exit gracefully with instructions (no hard crash).
# ============================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
create_update_backup_delete.py — v2.3.3 (Stable)
Maintains series data by fetching, updating, and backing up metadata and images.

Author: Generated by assistant (adapted from user's original script)
Last Updated: autogenerated
Notes:
 - This script is a careful, reordered version of the user's original script with
   enhancements: site-priority, grouped fetch blocks, metadata backups, 1-time fetching rules,
   enhanced report format, scriptVersion tagging, and cleanup of old backups.
 - Keep credentials and EXCEL_FILE_ID.txt as in the original workflow.
"""

# --------------------------- VERSION & SITE PRIORITY ------------------------
SCRIPT_VERSION = "v2.4.2 (Stable)"

# SITE_PRIORITY_BY_LANGUAGE controls which site is preferred for each fetched property
SITE_PRIORITY_BY_LANGUAGE = {
    "korean": {
        "synopsis": "asianwiki",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "chinese": {
        "synopsis": "mydramalist",
        "image": "mydramalist",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "japanese": {
        "synopsis": "asianwiki",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "asianwiki"
    },
    "thai": {
        "synopsis": "mydramalist",
        "image": "mydramalist",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "taiwanese": {
        "synopsis": "mydramalist",
        "image": "mydramalist",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "mydramalist"
    },
    "default": {
        "synopsis": "mydramalist",
        "image": "asianwiki",
        "otherNames": "mydramalist",
        "duration": "mydramalist",
        "releaseDate": "asianwiki"
    }
}


# ---------------------------- Human-readable field name mapping ----------------
FIELD_NAME_MAP = {
    "showName": "Show Name",
    "showImage": "Show Image",
    "otherNames": "Other Names",
    "watchStartedOn": "Watch Started On",
    "watchEndedOn": "Watch Ended On",
    "releasedYear": "Released Year",
    "releaseDate": "Release Date",
    "totalEpisodes": "Total Episodes",
    "showType": "Show Type",
    "nativeLanguage": "Native Language",
    "watchedLanguage": "Watched Language",
    "country": "Country",
    "comments": "Comments",
    "ratings": "Ratings",
    "genres": "Category",
    "network": "Network",
    "againWatchedDates": "Again Watched Dates",
    "updatedOn": "Updated On",
    "updatedDetails": "Updated Details",
    "synopsis": "Synopsis",
    "topRatings": "Top Ratings",
    "Duration": "Duration"
}

def human_readable_field(field):
    if not field:
        return field
    if field in FIELD_NAME_MAP:
        return FIELD_NAME_MAP[field]
    # split camelCase or snake_case
    parts = re.sub(r'[_\-]+', ' ', field)
    parts = re.sub(r'([a-z])([A-Z])', r'\1 \2', parts)
    parts = parts.split()
    return " ".join([p.capitalize() for p in parts])

# ---------------------------- Imports & Globals ----------------------------
import os
import re
import sys
import time
import json
import io
import shutil
import traceback
from datetime import datetime, timedelta, timezone

import pandas as pd
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

try:
    from ddgs import DDGS
    HAVE_DDGS = True
except Exception:
    HAVE_DDGS = False

# Try Google APIs optionally (same as original)
try:
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload, HttpRequest
    HAVE_GOOGLE_API = True
except Exception:
    HAVE_GOOGLE_API = False

# Timezone: IST (match original helper)
IST = timezone(timedelta(hours=5, minutes=30))

def now_ist():
    return datetime.now(IST)

def filename_timestamp():
    return now_ist().strftime("%d_%B_%Y_%H%M")

# Paths and config (preserve original values)
JSON_FILE = "seriesData.json"
BACKUP_DIR = "backups"
IMAGES_DIR = "images"
DELETE_IMAGES_DIR = "deleted-images"
DELETED_DATA_DIR = "deleted-data"
REPORTS_DIR = "reports"
PROGRESS_DIR = ".progress"
PROGRESS_FILE = os.path.join(PROGRESS_DIR, "progress.json")
MANUAL_UPDATE_REPORT = os.path.join(REPORTS_DIR, 'manual_update_report.json')
STATUS_JSON = os.path.join(REPORTS_DIR, "status.json")
BACKUP_META_DIR = "backup-meta-data"

# Ensure all necessary folders exist at startup
for _d in [BACKUP_DIR, IMAGES_DIR, DELETE_IMAGES_DIR, DELETED_DATA_DIR, REPORTS_DIR, BACKUP_META_DIR]:
    os.makedirs(_d, exist_ok=True)


EXCEL_FILE_ID_TXT = "EXCEL_FILE_ID.txt"
SERVICE_ACCOUNT_FILE = "GDRIVE_SERVICE_ACCOUNT.json"

GITHUB_PAGES_URL = os.environ.get("GITHUB_PAGES_URL", "").strip() or "https://<your-username>.github.io/my-movie-database"
MAX_PER_RUN = int(os.environ.get("MAX_PER_RUN", "0") or 0)
MAX_RUN_TIME_MINUTES = int(os.environ.get("MAX_RUN_TIME_MINUTES", "0") or 0)
KEEP_OLD_IMAGES_DAYS = int(os.environ.get("KEEP_OLD_IMAGES_DAYS", "7") or 7)
SCHEDULED_RUN = os.environ.get("SCHEDULED_RUN", "false").lower() == "true"
DEBUG_FETCH = os.environ.get("DEBUG_FETCH", "false").lower() == "true"
SYNOPSIS_MAX_LEN = int(os.environ.get("SYNOPSIS_MAX_LEN", "1000") or 1000)
METADATA_BACKUP_RETENTION_DAYS = int(os.environ.get("METADATA_BACKUP_RETENTION_DAYS", "90") or 90)

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; Bot/1.0)"}

def logd(msg):
    if DEBUG_FETCH:
        print("[DEBUG]", msg)

# ---------------------------- Utilities -------------------------------------
def safe_filename(name):
    return re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())

def ddmmyyyy(val):
    if pd.isna(val):
        return None
    if isinstance(val, pd.Timestamp):
        return val.strftime("%d-%m-%Y")
    s = str(val).strip()
    try:
        dt = pd.to_datetime(s, dayfirst=True, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.strftime("%d-%m-%Y")
    except Exception:
        return None

def load_progress():
    os.makedirs(PROGRESS_DIR, exist_ok=True)
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_progress(progress):
    os.makedirs(PROGRESS_DIR, exist_ok=True)
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, indent=2)

def normalize_whitespace_and_sentences(s):
    if not s:
        return s
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"\.([^\s])", r". \1", s)
    return s

def normalize_list_from_csv(cell_value, cap=False, strip=False):
    if cell_value is None:
        return []
    if isinstance(cell_value, (list, tuple)):
        items = [str(x) for x in cell_value if x is not None and str(x).strip()]
    else:
        s = str(cell_value)
        if not s.strip():
            return []
        items = [p for p in [p.strip() for p in s.split(",")] if p]
    if cap:
        items = [p.capitalize() if p else p for p in items]
    if strip:
        items = [p.strip() for p in items]
    return items

# ---------------------------- Date helpers ---------------------------------
_MONTHS = {m.lower(): m for m in ["January", "February", "March", "April", "May", "June",
                                  "July", "August", "September", "October", "November", "December"]}
_SHORT_MONTHS = {m[:3].lower(): m for m in _MONTHS}

def _normalize_month_name(m):
    mk = m.strip().lower()
    if mk in _MONTHS:
        return _MONTHS[mk]
    if mk in _SHORT_MONTHS:
        return _SHORT_MONTHS[mk]
    return m.capitalize()

def format_date_str(s):
    if not s:
        return None
    s = s.strip()
    m = re.search(r"([A-Za-z]+)\s+(\d{1,2}),\s*(\d{4})", s)
    if m:
        month = _normalize_month_name(m.group(1))
        day = str(int(m.group(2)))
        year = m.group(3)
        return f"{day} {month} {year}"
    m2 = re.search(r"(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})", s)
    if m2:
        day = str(int(m2.group(1)))
        month = _normalize_month_name(m2.group(2))
        year = m2.group(3)
        return f"{day} {month} {year}"
    return None

def format_date_range(s):
    if not s:
        return None
    m = re.search(r"([A-Za-z0-9,\s]+?)\s*[\-–]\s*([A-Za-z0-9,\s]+)", s)
    if m:
        d1 = format_date_str(m.group(1))
        d2 = format_date_str(m.group(2))
        if d1 and d2:
            return f"{d1} - {d2}"
    d = format_date_str(s)
    if d:
        return d
    return None

# ---------------------------- HTTP helpers ---------------------------------
def fetch_page(url, timeout=12):
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        if r.status_code == 200:
            return r.text
    except Exception as e:
        logd(f"fetch page error: {e}")
    return None

def try_ddgs_text(query, max_results=6):
    if not HAVE_DDGS:
        return []
    try:
        with DDGS() as dd:
            return list(dd.text(query, max_results=max_results))
    except Exception as e:
        logd(f"DDGS text error: {e}")
        return []

def ddgs_images(query, max_results=6):
    if not HAVE_DDGS:
        return []
    try:
        with DDGS() as dd:
            return [r.get("image") for r in dd.images(query, max_results=max_results) if r.get("image")]
    except Exception as e:
        logd(f"DDGS image error: {e}")
        return []

# ---------------------------- Image helpers --------------------------------
def download_image_to(url, path):
    try:
        r = requests.get(url, headers=HEADERS, timeout=12)
        if r.status_code == 200 and r.headers.get("content-type", "").startswith("image"):
            img = Image.open(BytesIO(r.content))
            img = img.convert("RGB")
            max_w, max_h = 600, 900
            img.thumbnail((max_w, max_h), Image.LANCZOS)
            os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            img.save(path, format="JPEG", quality=90)
            return True
    except Exception as e:
        logd(f"image download failed: {e}")
    return False

def build_absolute_url(local_path):
    local_path = local_path.replace("\\", "/")
    return GITHUB_PAGES_URL.rstrip("/") + "/" + local_path.lstrip("/")

def save_image_locally_from_url(img_url, show_id):
    """Save image found online into IMAGES_DIR with show_id as filename. Returns (local_path, remote_url)"""
    if not img_url:
        return None, None
    try:
        local_name = f"{show_id}.jpg" if show_id else safe_filename(img_url.split("/")[-1])
        local_path = os.path.join(IMAGES_DIR, local_name)
        ok = download_image_to(img_url, local_path)
        if ok:
            return local_path, build_absolute_url(local_path)
    except Exception as e:
        logd(f"save_image_locally_from_url error: {e}")
    return None, None

# ---------------------------- Parsing helpers from original -----------------
def clean_parenthesis_remove_cjk(s):
    if not s:
        return s
    return re.sub(r'\([^)]*[\u4e00-\u9fff\u3400-\u4dbf\uac00-\ud7af][^)]*\)', '', s)

def parse_synopsis_from_html(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    full_text = soup.get_text("\n", strip=True)
    syn_candidates = []
    meta = soup.find("meta", attrs={"name": "description"}) or soup.find("meta", attrs={"property": "og:description"})
    if meta and meta.get("content") and len(meta.get("content")) > 30:
        syn_candidates.append(meta.get("content").strip())
    for h in soup.find_all(re.compile("^h[1-6]$")):
        txt = h.get_text(" ", strip=True).lower()
        if any(k in txt for k in ("plot", "synopsis", "story", "summary")):
            parts = []
            for sib in h.find_next_siblings():
                if sib.name and re.match(r'^h[1-6]$', sib.name.lower()):
                    break
                if sib.name == 'p':
                    parts.append(sib.get_text(" ", strip=True))
                if sib.name in ('div', 'section'):
                    txt_inner = sib.get_text(" ", strip=True)
                    if txt_inner:
                        parts.append(txt_inner)
                if len(parts) >= 6:
                    break
            if parts:
                syn_candidates.append("\n\n".join(parts))
                break
    if not syn_candidates:
        for p in soup.find_all('p'):
            txt = p.get_text(" ", strip=True)
            if len(txt) > 80:
                syn_candidates.append(txt)
                break
    syn = syn_candidates[0] if syn_candidates else None
    duration = None
    try:
        lower = full_text.lower()
        m = re.search(r'(\b\d{2,3})\s*(?:min|minutes)\b', lower)
        if m:
            duration = int(m.group(1))
        else:
            m2 = re.search(r'runtime[^0-9]*(\d{1,3})', lower)
            if m2:
                duration = int(m2.group(1))
    except Exception:
        duration = None

    metadata = {}
    m = re.search(r'Also\s+Known\s+As[:\s]*([^\n\r]+)', full_text, flags=re.I)
    if m:
        other_raw = m.group(1).strip()
        metadata['otherNames'] = [p.strip() for p in re.split(r',\s*', other_raw) if p.strip()]
    else:
        metadata['otherNames'] = []

    m3 = re.search(r'(Release\s+Date|Aired|Aired on|Original release)[:\s]*([^\n\r]+)', full_text, flags=re.I)
    if m3:
        raw = m3.group(2).strip()
        rfmt = format_date_range(raw)
        if rfmt:
            metadata['releaseDateRaw'] = raw
            metadata['releaseDate'] = rfmt
        else:
            metadata['releaseDateRaw'] = raw
            metadata['releaseDate'] = raw
    else:
        m4 = re.search(r'([A-Za-z]+\s+\d{1,2},\s*\d{4})', full_text)
        if m4:
            metadata['releaseDateRaw'] = m4.group(1).strip()
            metadata['releaseDate'] = format_date_str(metadata['releaseDateRaw'])
        else:
            metadata['releaseDate'] = None

    if syn:
        syn = clean_parenthesis_remove_cjk(syn)
        paragraphs = [normalize_whitespace_and_sentences(p) for p in syn.split('\n\n') if p.strip()]
        syn = '\n\n'.join(paragraphs)
    domain = re.sub(r'^https?://(www\.)?', '', base_url).split('/')[0] if base_url else ''
    label = 'AsianWiki' if 'asianwiki' in domain else ('MyDramaList' if 'mydramalist' in domain else domain)
    syn_with_src = f"{syn} (Source: {label})" if syn else None
    return syn_with_src, duration, full_text, metadata

def pick_best_result(results):
    if not results:
        return None
    for r in results:
        url = r.get("href") or r.get("url") or r.get("link") or ""
        if any(site in url for site in ["mydramalist.com", "asianwiki.com", "wikipedia.org"]):
            return url
    return results[0].get("href") or results[0].get("url") or None

# ---------------------------- Grouped Fetch Blocks -------------------------
# Each of these groups contains 5 functions (synopsis, image, otherNames, duration, releaseDate)
# They use show_name + release_year to build queries/urls for higher accuracy.
# You can edit these blocks independently if a site changes layout.

# ============================================================
# 🏮 ASIANWIKI FETCHING BLOCKS
# ============================================================

def build_asianwiki_url(show_name, release_year=None):
    # AsianWiki pages often use underscored titles with year in parentheses for disambiguation.
    base = "https://asianwiki.com/"
    title = safe_filename(show_name).replace("_", " ").strip().replace(" ", "_")
    if release_year:
        # Try common pattern: Title_(YYYY)
        return base + f"{title}_({release_year})"
    return base + title

def fetch_synopsis_from_asianwiki(show_name, release_year):
    """Fetch synopsis from AsianWiki using show_name + release_year"""
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        # fallback to search via DDGS
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        return None, None, None
    syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
    return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', [])

def fetch_image_from_asianwiki(show_name, release_year, show_id):
    """Fetch image (poster) from AsianWiki page"""
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        # try ddgs image search
        imgs = ddgs_images(f"{show_name} {release_year} site:asianwiki.com")
        for img in imgs:
            local_path, remote = save_image_locally_from_url(img, show_id)
            if local_path:
                return local_path, remote, "asianwiki"
        return None, None, None
    soup = BeautifulSoup(html, "lxml")
    og = soup.find('meta', property='og:image')
    img_url = og.get('content') if og and og.get('content') else None
    if not img_url:
        img_tag = soup.find('img')
        img_url = img_tag.get('src') if img_tag and img_tag.get('src') else None
    if img_url and img_url.startswith('//'):
        img_url = 'https:' + img_url
    if img_url and img_url.startswith('/'):
        base = re.match(r'^(https?://[^/]+)', url)
        if base:
            img_url = base.group(1) + img_url
    if img_url:
        local_path, remote = save_image_locally_from_url(img_url, show_id)
        if local_path:
            return local_path, remote, "asianwiki"
    return None, None, None

def fetch_othernames_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        results = try_ddgs_text(f"{show_name} {release_year} site:asianwiki.com")
        url = pick_best_result(results) if results else None
        if url:
            html = fetch_page(url)
    if not html:
        return []
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('otherNames', [])

def fetch_duration_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        return None
    _, dur, _, _ = parse_synopsis_from_html(html, url)
    return dur

def fetch_release_date_from_asianwiki(show_name, release_year):
    url = build_asianwiki_url(show_name, release_year)
    html = fetch_page(url)
    if not html:
        return None
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('releaseDate')

# ============================================================
# 🌏 MYDRAMALIST FETCHING BLOCKS
# ============================================================

def build_mydramalist_url(show_name, release_year=None):
    # MyDramaList uses URL slugs often like /show/<id>-<slug> but we attempt search queries
    # We will rely on DDGS search to find the best page when exact slug not known.
    query = f"{show_name} {release_year} site:mydramalist.com" if release_year else f"{show_name} site:mydramalist.com"
    results = try_ddgs_text(query)
    url = pick_best_result(results) if results else None
    return url

def fetch_synopsis_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None, None, None
    html = fetch_page(url)
    if not html:
        return None, None, None
    syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
    return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', [])

def fetch_image_from_mydramalist(show_name, release_year, show_id):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        # fallback to ddgs images
        imgs = ddgs_images(f"{show_name} {release_year} site:mydramalist.com")
        for img in imgs:
            local_path, remote = save_image_locally_from_url(img, show_id)
            if local_path:
                return local_path, remote, "mydramalist"
        return None, None, None
    html = fetch_page(url)
    if not html:
        return None, None, None
    soup = BeautifulSoup(html, "lxml")
    og = soup.find('meta', property='og:image')
    img_url = og.get('content') if og and og.get('content') else None
    if not img_url:
        img_tag = soup.find('img')
        img_url = img_tag.get('src') if img_tag and img_tag.get('src') else None
    if img_url and img_url.startswith('//'):
        img_url = 'https:'+img_url
    local_path, remote = save_image_locally_from_url(img_url, show_id)
    if local_path:
        return local_path, remote, "mydramalist"
    return None, None, None

def fetch_othernames_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return []
    html = fetch_page(url)
    if not html:
        return []
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('otherNames', [])

def fetch_duration_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None
    html = fetch_page(url)
    if not html:
        return None
    _, dur, _, _ = parse_synopsis_from_html(html, url)
    return dur

def fetch_release_date_from_mydramalist(show_name, release_year):
    url = build_mydramalist_url(show_name, release_year)
    if not url:
        return None
    html = fetch_page(url)
    if not html:
        return None
    _, _, fulltext, metadata = parse_synopsis_from_html(html, url)
    return metadata.get('releaseDate')

# ---------------------------- Higher-level fetch orchestrators -------------
def fetch_synopsis_and_duration(show_name, release_year, prefer_sites=None, existing_synopsis=None, allow_replace=False, site_priority=None):
    """
    Orchestrates synopsis and duration fetching using preference order.
    Returns (synopsis, duration, releaseDate, otherNames, site_used_for_synopsis)
    """
    # If existing_synopsis is present and allow_replace is False and not a scheduled run, avoid overwriting synopsis.
    # BUT we still want to fetch releaseDate/otherNames/duration for new objects or if those fields are missing.
    if existing_synopsis and not allow_replace:
        # we will not overwrite synopsis, but may still attempt to fetch other metadata if site_priority indicates and if missing.
        pass

    # Determine prefer order from site_priority and prefer_sites
    prefer_list = []
    # if prefer_sites include unknown platform names try a ddgs search first
    if prefer_sites and HAVE_DDGS:
        unknowns = [p for p in prefer_sites if p and p.lower() not in ('asianwiki','mydramalist')]
        if unknowns:
            try:
                q = f"{show_name} {release_year or ''} {' '.join(unknowns)}"
                results = try_ddgs_text(q, max_results=6)
                url = pick_best_result(results) if results else None
                if url:
                    html = fetch_page(url)
                    if html:
                        syn, dur, fulltext, metadata = parse_synopsis_from_html(html, url)
                        return syn, dur, metadata.get('releaseDate'), metadata.get('otherNames', []), 'ddgs_search'
            except Exception as e:
                logd(f"ddgs prefer_sites search failed: {e}")
    if site_priority and isinstance(site_priority, dict):
        # Put explicitly preferred synopsis site first, then fallbacks
        primary = site_priority.get("synopsis")
        if primary:
            prefer_list.append(primary)
    # append provided prefer_sites (fallbacks)
    if prefer_sites:
        for p in prefer_sites:
            if p not in prefer_list:
                prefer_list.append(p)
    # default fallbacks
    for p in ("asianwiki", "mydramalist"):
        if p not in prefer_list:
            prefer_list.append(p)

    # Try each preferred site in order for synopsis/duration/release/otherNames
    for site in prefer_list:
        try:
            if site == "asianwiki":
                syn, dur, rdate, other = fetch_synopsis_from_asianwiki(show_name, release_year)
            elif site == "mydramalist":
                syn, dur, rdate, other = fetch_synopsis_from_mydramalist(show_name, release_year)
            else:
                syn = dur = rdate = None
                other = []
            if syn:
                return syn, dur, rdate, other, site
            # if no synopsis found, keep trying fallbacks but remember metadata if present
            if rdate or other or dur:
                # return whatever we found (prefer to get metadata even if synopsis missing)
                return syn, dur, rdate, other, site
        except Exception as e:
            logd(f"fetch_synopsis_and_duration site {site} failed: {e}")
    # If nothing found, return existing_synopsis or placeholder
    return (existing_synopsis or "Synopsis not available."), None, None, [], None

def fetch_and_save_image_for_show(show_name, prefer_sites, show_id, site_priority=None):
    """
    Orchestrates image fetching using site_priority preference. Returns (local_path, remote_url, site_used)
    """
    # Determine imaging preference
    prefer_list = []
    if site_priority and isinstance(site_priority, dict):
        primary = site_priority.get("image")
        if primary:
            prefer_list.append(primary)
    if prefer_sites:
        for p in prefer_sites:
            if p not in prefer_list:
                prefer_list.append(p)
    for p in ("asianwiki", "mydramalist"):
        if p not in prefer_list:
            prefer_list.append(p)

    # Try in order
    for site in prefer_list:
        try:
            if site == "asianwiki":
                lp, ru, used = fetch_image_from_asianwiki(show_name, None, show_id)
            elif site == "mydramalist":
                lp, ru, used = fetch_image_from_mydramalist(show_name, None, show_id)
            else:
                lp = ru = used = None
            if lp:
                return lp, ru, used
        except Exception as e:
            logd(f"fetch_and_save_image_for_show site {site} failed: {e}")
    # Last resort: ddgs image results
    try:
        imgs = ddgs_images(f"{show_name} {release_year if 'release_year' in locals() else ''}")
        for img in imgs:
            if img:
                local_path, remote = save_image_locally_from_url(img, show_id)
                if local_path:
                    return local_path, remote, "ddgs"
    except Exception as e:
        logd(f"ddgs_images fallback failed: {e}")
    return None, None, None

# ---------------------------- Deletion processing (kept original logic) ---
def process_deletions(excel_file, json_file, report_changes):
    try:
        df = pd.read_excel(excel_file, sheet_name='Deleting Records')
    except Exception:
        return [], []
    if df.shape[1] < 1:
        return [], []
    cols = [str(c).strip().lower() for c in df.columns]
    id_col = None
    for i, c in enumerate(cols):
        if c == 'id' or 'id' in c:
            id_col = df.columns[i]
            break
    if id_col is None:
        id_col = df.columns[0]
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as jf:
                data = json.load(jf)
        except Exception:
            data = []
    else:
        data = []
    by_id = {int(o['showID']): o for o in data if 'showID' in o and isinstance(o['showID'], int)}
    to_delete = []
    for _, row in df.iterrows():
        val = row[id_col]
        if pd.isna(val):
            continue
        try:
            to_delete.append(int(val))
        except Exception:
            continue
    if not to_delete:
        return [], []
    os.makedirs(DELETED_DATA_DIR, exist_ok=True)
    deleted_ids = []
    not_found_ids = []
    for iid in to_delete:
        if iid in by_id:
            deleted_obj = by_id.pop(iid)
            deleted_ids.append(iid)
            fname = f"DELETED_{now_ist().strftime('%d_%B_%Y_%H%M')}_{iid}.json"
            outpath = os.path.join(DELETED_DATA_DIR, safe_filename(fname))
            try:
                with open(outpath, 'w', encoding='utf-8') as of:
                    json.dump(deleted_obj, of, indent=4, ensure_ascii=False)
                report_changes.setdefault('deleted', []).append(
    f"{iid} -> {deleted_obj.get('showName', 'Unknown')} ({deleted_obj.get('releasedYear', 'N/A')}) -> ✅ Deleted and archived -> {outpath}"
)

                try:
                    img_url = deleted_obj.get('showImage') or ""
                    if img_url:
                        candidate = None
                        m = re.search(r'/(images/[^/?#]+)$', img_url)
                        if m:
                            candidate = m.group(1)
                        elif img_url.startswith('images/'):
                            candidate = img_url
                        if candidate:
                            src = os.path.join('.', candidate)
                            if os.path.exists(src):
                                os.makedirs(DELETE_IMAGES_DIR, exist_ok=True)
                                dst_name = f"{iid}_{filename_timestamp()}.jpg"
                                dst = os.path.join(DELETE_IMAGES_DIR, safe_filename(dst_name))
                                shutil.move(src, dst)
                                report_changes.setdefault('deleted_images_moved', []).append(
                                    f"{iid} -> image moved: {candidate} -> {dst}"
                                )
                except Exception as e_img:
                    report_changes.setdefault('deleted_images_moved', []).append(f"{iid} -> ⚠️ Image move failed: {e_img}")
            except Exception as e:
                report_changes.setdefault('deleted', []).append(f"{iid} -> ⚠️ Deletion recorded but failed to write archive: {e}")
        else:
            not_found_ids.append(iid)
            report_changes.setdefault('deleted_not_found', []).append(f"-{iid} -> ❌ Not found in seriesData.json")
    merged = sorted(by_id.values(), key=lambda x: x.get('showID', 0))
    try:
        with open(json_file, 'w', encoding='utf-8') as jf:
            json.dump(merged, jf, indent=4, ensure_ascii=False)
        report_changes.setdefault('deleted_summary', []).append(
            f"seriesData.json updated after deletions (deleted {len(deleted_ids)} items)."
        )
    except Exception as e:
        report_changes.setdefault('deleted_summary', []).append(f"⚠️ Failed to write updated {json_file}: {e}")
    return deleted_ids, not_found_ids

# ---------------------------- Manual Updates (preserve original) ----------

def apply_manual_updates(excel_file: str, json_file: str):
    sheet = 'manual update'
    try:
        df = pd.read_excel(excel_file, sheet_name=sheet)
    except Exception:
        print("ℹ️ No 'manual update' sheet found; skipping manual updates.")
        return

    if df.shape[1] < 2:
        print("Manual update sheet must have at least two columns: showID and dataString")
        return

    if not os.path.exists(json_file):
        print("No JSON file to update")
        return

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception:
        data = []

    by_id = {o['showID']: o for o in data if 'showID' in o}
    updated_objs = []

    for _, row in df.iterrows():
        sid = None
        try:
            sid = int(row[0]) if not pd.isna(row[0]) else None
        except Exception:
            continue
        if sid is None or sid not in by_id:
            continue

        raw = row[1]
        if pd.isna(raw) or not str(raw).strip():
            continue

        s = str(raw).strip()
        try:
            if s.startswith('{') and s.endswith('}'):
                upd = json.loads(s)
            else:
                if s.startswith('{') and not s.endswith('}'):
                    s = s + '}'
                if not s.startswith('{'):
                    s2 = '{' + s + '}'
                else:
                    s2 = s
                upd = json.loads(s2)
        except Exception:
            upd = {}
            parts = [p.strip() for p in s.split(',') if p.strip()]
            for part in parts:
                if ':' in part:
                    k, v = part.split(':', 1)
                    upd[k.strip()] = v.strip()

        if not upd:
            continue

        obj = by_id[sid]

        # --- Apply updates ---
        for k, v in upd.items():
            if k.lower() == "ratings":
                try:
                    obj["ratings"] = int(v)
                except Exception:
                    obj["ratings"] = obj.get("ratings", 0)
            elif k.lower() in ("releasedyear", "year"):
                try:
                    obj["releasedYear"] = int(v)
                except Exception:
                    pass
            else:
                obj[k] = v

            # Handle site priority for manual update
            mapping = {
                'showImage': 'image',
                'releaseDate': 'releaseDate',
                'otherNames': 'otherNames',
                'Duration': 'duration',
                'synopsis': 'synopsis'
            }
            key_for_site = mapping.get(k)
            if key_for_site:
                spu = obj.get('sitePriorityUsed') or {}
                spu[key_for_site] = 'Manual'
                obj['sitePriorityUsed'] = spu

        # --- Set update metadata ---
        obj['updatedOn'] = now_ist().strftime("%d %B %Y, %I:%M %p (IST)")
        obj['updatedDetails'] = f"Updated {', '.join([k.capitalize() for k in upd.keys()])} Manually By Owner"

        updated_objs.append(obj)

    if updated_objs:
        merged = sorted(by_id.values(), key=lambda x: x.get('showID', 0))
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(merged, f, indent=4, ensure_ascii=False)
        print(f"✅ Applied {len(updated_objs)} manual updates")
    else:
        print("ℹ️ No valid manual updates found/applied.")
def tidy_comment(val):
    if pd.isna(val) or not str(val).strip():
        return None
    text = re.sub(r'\s+', ' ', str(val)).strip()
    if not text.endswith('.'):
        text = text + '.'
    text = re.sub(r'\.([^\s])', r'. \1', text)
    return text

def sheet_base_offset(sheet_name: str) -> int:
    if sheet_name == "Sheet1":
        return 100
    if sheet_name == "Feb 7 2023 Onwards":
        return 1000
    if sheet_name == "Sheet2":
        return 3000
    return 0

# ---------------------------- Helper: save metadata backup ------------------
def save_metadata_backup(show_id, show_name, language, fetched_fields, site_priority_used):
    try:
        os.makedirs(BACKUP_META_DIR, exist_ok=True)
        fname = f"META_{now_ist().strftime('%d_%B_%Y_%H%M')}_{show_id}.json"
        outpath = os.path.join(BACKUP_META_DIR, safe_filename(fname))
        payload = {
            "scriptVersion": SCRIPT_VERSION,
            "showID": show_id,
            "showName": show_name,
            "language": language,
            "timestamp": now_ist().strftime("%d %B %Y %I:%M %p (IST)"),
            "fetchedFields": fetched_fields,
            "sitePriorityUsed": site_priority_used
        }
        with open(outpath, 'w', encoding='utf-8') as of:
            json.dump(payload, of, indent=2, ensure_ascii=False)
        return outpath
    except Exception as e:
        logd(f"save_metadata_backup failed: {e}")
        return None


# ---------------------------- Helper: backup before modification ------------------

# ---------------------------- Helper: backup before modification ------------------

def objects_differ(old, new, ignore_fields=None):
    """Return True if objects differ on any key except ignore_fields."""
    if ignore_fields is None:
        ignore_fields = {'updatedOn', 'updatedDetails', 'topRatings'}
    if not isinstance(old, dict) or not isinstance(new, dict):
        return True
    keys = set(old.keys()) | set(new.keys())
    for k in keys:
        if k in ignore_fields:
            continue
        if old.get(k) != new.get(k):
            return True
    return False

def backup_before_modification(show_id, old_obj):
    """Save the *old* object to BACKUP_DIR as BEFORE_<timestamp>_<show_id>.json
    only when an actual modification occurs. Returns the backup path or None.
    """
    try:
        if not old_obj or not isinstance(old_obj, dict):
            return None
        os.makedirs(BACKUP_DIR, exist_ok=True)
        fname = f"BEFORE_{now_ist().strftime('%d_%B_%Y_%H%M')}_{show_id}.json"
        outpath = os.path.join(BACKUP_DIR, safe_filename(fname))
        with open(outpath, 'w', encoding='utf-8') as of:
            json.dump(old_obj, of, indent=4, ensure_ascii=False)
        return outpath
    except Exception as e:
        logd(f"backup_before_modification failed for {show_id}: {e}")
        return None


# ---------------------------- Cleanup metadata backups ---------------------

# ---------------------------- Helper: move deleted image --------------------
def move_deleted_image(show_id, img_path):
    """Move an image file for a deleted object into DELETE_IMAGES_DIR.
    Returns destination path on success, None otherwise. Creates the directory if needed.
    Safe and idempotent: if file not found or move fails, it logs and returns None.
    """
    try:
        if not img_path or not os.path.exists(img_path):
            return None
        os.makedirs(DELETE_IMAGES_DIR, exist_ok=True)
        # create a safe destination filename using show_id and timestamp
        dst_name = f"{show_id}_{filename_timestamp()}.jpg"
        dst_path = os.path.join(DELETE_IMAGES_DIR, safe_filename(dst_name))
        # If destination already exists, append a counter
        counter = 1
        base, ext = os.path.splitext(dst_path)
        while os.path.exists(dst_path):
            dst_path = f"{base}_{counter}{ext}"
            counter += 1
        shutil.move(img_path, dst_path)
        return dst_path
    except Exception as e:
        try:
            logd(f"move_deleted_image failed for {show_id}: {e}")
        except Exception:
            pass
        return None


def cleanup_old_metadata_backups(retention_days=METADATA_BACKUP_RETENTION_DAYS):
    if not os.path.exists(BACKUP_META_DIR):
        return 0
    cutoff = datetime.now() - timedelta(days=retention_days)
    removed = 0
    for fname in os.listdir(BACKUP_META_DIR):
        path = os.path.join(BACKUP_META_DIR, fname)
        try:
            mtime = datetime.fromtimestamp(os.path.getmtime(path))
            if mtime < cutoff:
                os.remove(path)
                removed += 1
                print(f"🧹 Removed expired metadata backup: {path}")
        except Exception as e:
            print(f"⚠️ Could not cleanup metadata backup {path}: {e}")
    return removed

# ---------------------------- Excel to objects (modified for one-time fetch) -
def excel_to_objects(excel_file, sheet_name, existing_by_id, report_changes, start_index=0, max_items=None, time_limit_seconds=None,
                     deleted_ids_for_run=None, deleting_not_found_initial=None, deleting_found_in_sheets=None):
    df = pd.read_excel(excel_file, sheet_name=sheet_name)
    df.columns = [c.strip().lower() for c in df.columns]
    again_idx = None
    for i, c in enumerate(df.columns):
        if "again watched" in c:
            again_idx = i
            break
    if again_idx is None:
        raise ValueError(f"'Again Watched Date' columns not found in sheet: {sheet_name}")
    items = []
    processed = 0
    start_time = time.time()
    last_idx = start_index
    total_rows = len(df)
    for idx in range(start_index, total_rows):
        if max_items and processed >= max_items:
            break
        if time_limit_seconds and (time.time() - start_time) >= time_limit_seconds:
            break
        row = df.iloc[idx]
        obj = {}
        try:
            for col in df.columns[:again_idx]:
                key = COLUMN_MAP.get(col, col)
                val = row[col]
                if key == "showID":
                    base = sheet_base_offset(sheet_name)
                    obj["showID"] = base + int(val) if pd.notna(val) else None
                elif key == "showName":
                    raw_name = str(val) if pd.notna(val) else ""
                    clean_name = re.sub(r'\s+', ' ', raw_name).strip()
                    obj["showName"] = clean_name if clean_name else None
                elif key in ("watchStartedOn", "watchEndedOn"):
                    obj[key] = ddmmyyyy(val)
                elif key == "releasedYear":
                    obj[key] = int(val) if pd.notna(val) else None
                elif key == "totalEpisodes":
                    obj[key] = int(val) if pd.notna(val) else None
                elif key == "nativeLanguage":
                    obj[key] = str(val).strip().capitalize() if pd.notna(val) else None
                elif key == "watchedLanguage":
                    obj[key] = str(val).strip().capitalize() if pd.notna(val) else None
                elif key == "comments":
                    obj[key] = tidy_comment(val)
                elif key == "ratings":
                    try:
                        obj[key] = int(val) if pd.notna(val) else 0
                    except Exception:
                        obj[key] = 0
                elif key == "genres":
                    obj[key] = normalize_list_from_csv(val, cap=False, strip=True)
                elif key == "network":
                    obj[key] = normalize_list_from_csv(val, cap=False, strip=True)
                else:
                    obj[key] = str(val).strip() if pd.notna(val) else None
            obj["showType"] = "Mini Drama" if sheet_name.lower() == "mini drama" else "Drama"
            obj["country"] = None
            native = obj.get("nativeLanguage")
            if native:
                n = str(native).strip().lower()
                if n in ("korean", "korea", "korean language"):
                    obj["country"] = "South Korea"
                elif n in ("chinese", "china", "mandarin"):
                    obj["country"] = "China"
                elif n in ("japanese", "japan"):
                    obj["country"] = "Japan"
            dates = [ddmmyyyy(v) for v in row[again_idx:] if ddmmyyyy(v)]
            obj["againWatchedDates"] = dates
            obj["updatedOn"] = now_ist().strftime("%d %B %Y")
            obj["updatedDetails"] = "First time Uploaded"
            r = int(obj.get("ratings") or 0)
            obj["topRatings"] = r * (len(dates) if len(dates) > 0 else 1) * 100
            obj.setdefault("otherNames", [])
            obj["Duration"] = None

            sid = obj.get("showID")
            if deleted_ids_for_run and sid in deleted_ids_for_run:
                report_changes.setdefault('ignored_deleting', []).append(
                    f'{sid} -> Already Deleted as per "Deleting Records" Sheet -> ⚠️ Cannot add to seriesData.json'
                )
                if deleting_not_found_initial and sid in deleting_not_found_initial:
                    deleting_found_in_sheets.add(sid)
                continue

            if deleting_not_found_initial and sid in deleting_not_found_initial:
                deleting_found_in_sheets.add(sid)

            show_name = obj.get("showName")
            released_year = obj.get("releasedYear")
            language = (obj.get("nativeLanguage") or "").strip().lower()
            site_priority = SITE_PRIORITY_BY_LANGUAGE.get(language, SITE_PRIORITY_BY_LANGUAGE["default"])

            existing = existing_by_id.get(obj.get("showID")) if obj.get("showID") is not None else None
            existing_image_url = existing.get("showImage") if existing else None
            new_image_url = existing_image_url or None

            # --- One-time fetch rule: if this is a NEW object (existing is None),
            # fetch synopsis, image, otherNames, releaseDate, Duration.
            metadata_backup_fields = {}
            site_priority_used = {}
            if existing is None:
                # Fetch image
                try:
                    local_image_path, remote_image_url, img_site = fetch_and_save_image_for_show(show_name, None, sid, site_priority=site_priority)
                    if local_image_path:
                        new_image_url = build_absolute_url(local_image_path)
                        report_changes.setdefault('images', []).append({'showID': sid, 'showName': show_name, 'old': existing_image_url, 'new': new_image_url})
                        metadata_backup_fields.setdefault("showImage", {"value": new_image_url, "source": img_site})
                        site_priority_used["image"] = img_site or site_priority.get("image")
                    else:
                        report_changes.setdefault('fetch_errors', []).append(f"{sid} - {show_name} ({released_year}) -> Can't Fetch Show Image")
                except Exception as e:
                    logd(f"Image fetch failed for {show_name}: {e}")

                # Fetch synopsis and duration and releaseDate and otherNames using site_priority
                try:
                    new_syn, dur, rdate, othernames, site_used = fetch_synopsis_and_duration(show_name, released_year, prefer_sites=None, existing_synopsis=None, allow_replace=False, site_priority=site_priority)
                    if new_syn and isinstance(new_syn, str):
                        new_syn = normalize_whitespace_and_sentences(new_syn)
                    obj["synopsis"] = new_syn
                    if rdate:
                        obj['releaseDate'] = format_date_range(rdate) or format_date_str(rdate) or rdate
                        metadata_backup_fields.setdefault("releaseDate", {"value": rdate, "source": site_used})
                        site_priority_used["releaseDate"] = site_used or site_priority.get("releaseDate")
                    if othernames:
                        obj['otherNames'] = othernames
                        metadata_backup_fields.setdefault("otherNames", {"value": othernames, "source": site_used})
                        site_priority_used["otherNames"] = site_used or site_priority.get("otherNames")
                    if dur is not None and dur > 0:
                        obj['Duration'] = int(dur)
                        metadata_backup_fields.setdefault("Duration", {"value": int(dur), "source": site_used})
                        site_priority_used["duration"] = site_used or site_priority.get("duration")
                    if new_syn:
                        metadata_backup_fields.setdefault("synopsis", {"value": new_syn, "source": site_used})
                        site_priority_used["synopsis"] = site_used or site_priority.get("synopsis")
                except Exception as e:
                    logd(f"Synopsis/metadata fetch failed for {show_name}: {e}")

                                # Save metadata backup (one-time)
                try:
                    os.makedirs(BACKUP_META_DIR, exist_ok=True)
                    backup_path = save_metadata_backup(sid, show_name, language, metadata_backup_fields or {}, site_priority_used or {})
                    if backup_path:
                        report_changes.setdefault('metadata_backups_created', []).append(f"💾 Metadata Backup Created: {backup_path}")
                    if site_priority_used:
                        obj['sitePriorityUsed'] = site_priority_used.copy()
                except Exception as e:
                    logd(f"metadata backup save failed: {e}")
                # If no metadata fields were collected, still create an empty metadata backup
                if not metadata_backup_fields:
                    try:
                        os.makedirs(BACKUP_META_DIR, exist_ok=True)
                        backup_path = save_metadata_backup(sid, show_name, language, {}, site_priority_used or {})
                        if backup_path and os.path.exists(backup_path):
                            report_changes.setdefault('metadata_backups_created', []).append(
                                f"💾 Metadata Backup Created: {backup_path}"
                            )
                        else:
                            report_changes.setdefault('metadata_backups_failed', []).append(
                                f"⚠️ Failed to create metadata backup for {sid}: File not found after save attempt"
                            )
                    except Exception as e:
                        report_changes.setdefault('metadata_backups_failed', []).append(
                            f"⚠️ Exception during metadata backup for {sid}: {e}"
                        )

                # Also populate sourceSites in main object (only for new shows)
                if site_priority_used:
                    obj['sourceSites'] = site_priority_used.copy()

            else:
                # Existing object: preserve existing synopsis, image, otherNames, etc. Do not overwrite.
                obj['synopsis'] = existing.get("synopsis") or existing.get("synopsis", None)
                obj['Duration'] = existing.get("Duration") or obj.get("Duration")
                obj['otherNames'] = existing.get("otherNames") or obj.get("otherNames", [])
                obj['showImage'] = existing.get("showImage") or obj.get("showImage")
                obj['releaseDate'] = existing.get("releaseDate") or None

                # Fill missing individual fields only if they are completely missing
                # (e.g., if an existing object has no showImage, we may fetch image)
                # For existing objects, we'll fetch missing fields only (not overwrite)
                # Image missing? fetch it
                if (existing_image_url is None) and (sid is not None):
                    try:
                        local_image_path, remote_image_url, img_site = fetch_and_save_image_for_show(show_name, None, sid, site_priority=site_priority)
                        if local_image_path:
                            new_image_url = build_absolute_url(local_image_path)
                            report_changes.setdefault('images', []).append({'showID': sid, 'showName': show_name, 'old': existing_image_url, 'new': new_image_url})
                            obj['showImage'] = new_image_url
                    except Exception as e:
                        logd(f"Existing object image fetch failed for {show_name}: {e}")

            obj["showImage"] = new_image_url

            ordered = {
                "showID": obj.get("showID"),
                "showName": obj.get("showName"),
                "otherNames": obj.get("otherNames", []),
                "showImage": obj.get("showImage"),
                "watchStartedOn": obj.get("watchStartedOn"),
                "watchEndedOn": obj.get("watchEndedOn"),
                "releasedYear": obj.get("releasedYear"),
                "releaseDate": obj.get("releaseDate"),
                "totalEpisodes": obj.get("totalEpisodes"),
                "showType": obj.get("showType"),
                "nativeLanguage": obj.get("nativeLanguage"),
                "watchedLanguage": obj.get("watchedLanguage"),
                "country": obj.get("country"),
                "comments": obj.get("comments"),
                "ratings": obj.get("ratings"),
                "genres": obj.get("genres"),
                "network": obj.get("network"),
                "againWatchedDates": obj.get("againWatchedDates"),
                "updatedOn": obj.get("updatedOn"),
                "updatedDetails": obj.get("updatedDetails"),
                "synopsis": obj.get("synopsis"),
                "topRatings": obj.get("topRatings"),
                "Duration": obj.get("Duration"),
                "sitePriorityUsed": obj.get("sitePriorityUsed", {})
            }
            items.append(ordered)
            processed += 1
            last_idx = idx
            sid = ordered.get("showID")
            if existing is None:
                report_changes.setdefault("created", []).append(ordered)
            else:
                # Precise change detection: ignore trivial fields like updatedOn/updatedDetails/topRatings
                try:
                    changed = objects_differ(existing, ordered)
                except Exception:
                    changed = (existing != ordered)
                if changed:
                    # Save BEFORE backup only when there is a real modification
                    try:
                        backup_before_modification(ordered.get('showID'), existing)
                    except Exception as _e:
                        logd(f"backup failed for {ordered.get('showID')}: {_e}")
                    report_changes.setdefault("updated", []).append({"old": existing, "new": ordered})
                else:
                    # Mark as skipped (unchanged)
                    report_changes.setdefault('skipped', []).append(f"{ordered.get('showID')} - {ordered.get('showName')} ({ordered.get('releasedYear')})")

        except Exception as e:
            raise RuntimeError(f"Row {idx} in sheet '{sheet_name}' processing failed: {e}")
    finished = (last_idx >= total_rows - 1) if total_rows > 0 else True
    next_index = (last_idx + 1) if processed > 0 else start_index
    return items, processed, finished, next_index

# ---------------------------- Reports --------------------------------------

def write_report(report_changes_by_sheet, report_path, final_not_found_deletions=None, start_time=None, end_time=None, metadata_backups_removed=0):
    lines = []
    # Header
    lines.append("✅ Workflow completed successfully")
    if end_time is None:
        end_time = now_ist()
    if start_time is None:
        start_time = end_time
    lines.append(f"📅 Run Time: {end_time.strftime('%d %B %Y %I:%M %p (IST)')}")
    duration_td = end_time - start_time
    seconds = int(duration_td.total_seconds())
    mins, secs = divmod(seconds, 60)
    lines.append(f"🕒 Duration: {mins} min {secs} sec")
    lines.append(f"⚙️ Script Version: {SCRIPT_VERSION}")
    lines.append("")
    sep = "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    total_created = total_updated = total_deleted = total_skipped = total_images_updated = total_warnings = total_failed = 0
    grand_rows = 0

    # Per-sheet details (skip per-sheet summary for Deleting Records and Manual Update)
    for sheet, changes in report_changes_by_sheet.items():
        # Skip empty Deleting Records / Manual Update sections unless they contain meaningful entries
        if sheet in ('Deleting Records', 'Manual Update'):
            meaningful = any(changes.get(k) for k in ('deleted','deleted_not_found','deleted_summary','error','metadata_backups_created','updated_objs'))
            if not meaningful:
                continue
            lines.append(sep)
            lines.append(f"🗂️ === {sheet} — {now_ist().strftime('%d %B %Y')} ===")
            lines.append(sep)
            if sheet == 'Deleting Records':
                if changes.get('deleted'):
                    lines.append("❌ Data Deleted:")
                    for d in changes.get('deleted'):
                        lines.append(f"- {d}")
                    lines.append("")
                if changes.get('deleted_not_found'):
                    lines.append("❌ Deletion notes (IDs not found in seriesData.json initially):")
                    for note in changes.get('deleted_not_found'):
                        lines.append(f"- {note}")
                    lines.append("")
                continue
            if sheet == 'Manual Update':
                if changes.get('updated_objs'):
                    lines.append("")
                    lines.append("🔁 Manual Updates Applied:")
                    for u in changes.get('updated_objs'):
                        lines.append(f"- {u}")
                if changes.get('metadata_backups_created'):
                    lines.append("")
                    lines.append("💾 Metadata Backups Created:")
                    for b in changes.get('metadata_backups_created'):
                        lines.append(f"- {b}")
                lines.append("")
                continue

        lines.append(sep)
        lines.append(f"🗂️ === {sheet} — {now_ist().strftime('%d %B %Y')} ===")
        lines.append(sep)
        created = changes.get('created', [])
        if created:
            lines.append("")
            lines.append("🆕 Data Created:")
            for obj in created:
                lines.append(f"- {obj.get('showID','N/A')} - {obj.get('showName','Unknown')} ({obj.get('releasedYear','N/A')}) -> First Time Uploaded")
        updated = changes.get('updated', [])
        if updated:
            lines.append("")
            lines.append("🔁 Data Updated:")
            for pair in updated:
                new = pair.get('new')
                old = pair.get('old')
                # compute changed fields and map to human readable names
                changed_fields = []
                for k in new.keys():
                    if old.get(k) != new.get(k):
                        if k in ('updatedOn','updatedDetails','topRatings'):
                            continue
                        changed_fields.append(human_readable_field(k))
                fields_text = ", ".join(changed_fields) + " Updated" if changed_fields else "Updated"
                lines.append(f"- {new.get('showID','N/A')} - {new.get('showName','Unknown')} ({new.get('releasedYear','N/A')}) -> {fields_text}")
        skipped = changes.get('skipped', [])
        if skipped:
            lines.append("")
            lines.append("🚫 Unchanged Entries (Skipped):")
            for name in skipped:
                lines.append(f"- {name}")
        images = changes.get('images', [])
        if images:
            lines.append("")
            lines.append("🖼️ Image Updated:")
            for itm in images:
                lines.append(f"- {itm.get('showID','N/A')} - {itm.get('showName','Unknown')} -> Updated Successfully")
                total_images_updated += 1
        if changes.get('metadata_backups_created'):
            lines.append("")
            lines.append("💾 Metadata Backups Created:")
            for b in changes.get('metadata_backups_created'):
                lines.append(f"- {b}")
        if changes.get('deleted'):
            lines.append("")
            lines.append("❌ Data Deleted:")
            for d in changes.get('deleted'):
                lines.append(f"- {d}")
        if changes.get('deleted_not_found'):
            lines.append("")
            lines.append("❌ Deletion notes (IDs not found in seriesData.json initially):")
            for note in changes.get('deleted_not_found'):
                lines.append(f"- {note}")
        if changes.get('fetch_errors'):
            lines.append("")
            lines.append("⚠️ Fetch Errors:")
            for fe in changes.get('fetch_errors'):
                lines.append(f"- {fe}")

        # summary per sheet
        lines.append("")
        lines.append(sep)
        lines.append(f"📊 Summary (Sheet: {sheet})")
        lines.append(sep)
        ccount = len(created)
        ucount = len(updated)
        scount = len(skipped) if skipped else 0
        total_created += ccount
        total_updated += ucount
        total_skipped += scount
        total_deleted += len(changes.get('deleted', [])) if changes.get('deleted') else 0
        rows_count = changes.get('rows_processed', None)
        # Exclude Deleting Records and Manual Update rows from grand total
        if sheet not in ('Deleting Records', 'Manual Update'):
            grand_rows += rows_count if rows_count else (ccount + ucount + scount)
        lines.append(f"🆕 Total Created: {ccount}")
        lines.append(f"🔁 Total Updated: {ucount}")
        lines.append(f"🚫 Total Skipped: {scount}")
        lines.append(f"🖼️ Total Images Updated: {len(images) if images else 0}")
        lines.append(f"⚠️ Total Warnings: {len(changes.get('warnings', [])) if changes.get('warnings') else 0}")
        lines.append(f"❌ Total Failed: {len(changes.get('failed', [])) if changes.get('failed') else 0}")
        lines.append(f"  Total Number of Rows: {rows_count if rows_count is not None else (ccount + ucount + scount)}")
        lines.append("")

    # Deletion summary aggregated if present (include only if meaningful)
    if 'Deleting Records' in report_changes_by_sheet:
        del_changes = report_changes_by_sheet['Deleting Records']
        meaningful = any(del_changes.get(k) for k in ('deleted','deleted_not_found','deleted_summary'))
        if meaningful:
            lines.append(sep)
            lines.append(f"🗑️ === Deleting Records — {now_ist().strftime('%d %B %Y')} ===")
            lines.append(sep)
            if del_changes.get('deleted'):
                lines.append("❌ Data Deleted:")
                for d in del_changes.get('deleted'):
                    lines.append(f"- {d}")
                lines.append("")

                # Corrected f-string — properly closed quotes and parentheses
                lines.append(
                    f"✅ Deleted archived items: {len(del_changes.get('deleted'))} "
                    f"(IDs: {', '.join([str(x).split(' ')[0] for x in del_changes.get('deleted')])})"
                )

                # Dynamically check for before/after backup JSON files
                try:
                    backup_files = [
                        f for f in os.listdir(BACKUP_DIR)
                        if f.startswith('before_') or f.startswith('after_')
                    ]
                    if backup_files:
                        before_files = [f for f in backup_files if f.startswith('before_')]
                        after_files = [f for f in backup_files if f.startswith('after_')]
                        before_list = ', '.join(before_files) if before_files else 'N/A'
                        after_list = ', '.join(after_files) if after_files else 'N/A'
                        lines.append(f"💾 Backup files saved: {before_list}, {after_list}")
                    else:
                        lines.append("💾 Backup files saved: None found")
                except Exception as e:
                    lines.append(f"💾 Backup files check failed: {e}")

                lines.append("")
    # overall summary
    lines.append(sep)
    lines.append("📊 Overall Summary")
    lines.append(sep)
    lines.append(f"🆕 Total Created: {total_created}")
    lines.append(f"🔁 Total Updated: {total_updated}")
    lines.append(f"🚫 Total Skipped: {total_skipped}")
    lines.append(f"❌ Total Deleted: {total_deleted}")
    lines.append(f"💾 Backup files: {len(os.listdir(BACKUP_DIR)) if os.path.exists(BACKUP_DIR) else 0}")
    lines.append(f"  Grand Total Rows Processed: {grand_rows}")

    # metadata backup cleanup summary
    lines.append("")
    lines.append(f"💾 Metadata Backups Created: {sum(len(ch.get('metadata_backups_created', [])) for ch in report_changes_by_sheet.values())}")
    lines.append(f"🧹 Cleaned up old metadata backups: {metadata_backups_removed} removed (older than {METADATA_BACKUP_RETENTION_DAYS} days)")
    lines.append("")

    # total objects in seriesData.json
    try:
        if os.path.exists(JSON_FILE):
            with open(JSON_FILE, 'r', encoding='utf-8') as jf:
                arr = json.load(jf)
                lines.append(f"📦 Total Objects in seriesData.json: {len(arr)}")
    except Exception:
        lines.append("📦 Total Objects in seriesData.json: Unknown (could not read file)")
    lines.append("\n⚠️ WARNING: No duplicate records detected.")
    lines.append("🏁 Workflow finished successfully")
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))
    except Exception as e:
        print(f"⚠️ Could not write TXT report: {e}")


# ---------------------------- Secret scan & email body ---------------------
def scan_for_possible_secrets():
    findings = []
    if os.path.exists(SERVICE_ACCOUNT_FILE):
        try:
            s = open(SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8').read()
            has_private_key = 'private_key' in s
            m = re.search(r'"client_email"\s*:\s*"([^"]+)"', s)
            client_email = m.group(1) if m else None
            findings.append({'file': SERVICE_ACCOUNT_FILE, 'present': True, 'client_email': client_email, 'has_private_key': bool(has_private_key), 'note': 'Service account JSON detected'})
        except Exception as e:
            findings.append({'file': SERVICE_ACCOUNT_FILE, 'present': True, 'note': f'Could not read file safely: {e}'})
    if os.path.exists(EXCEL_FILE_ID_TXT):
        try:
            s = open(EXCEL_FILE_ID_TXT, 'r', encoding='utf-8').read().strip()
            findings.append({'file': EXCEL_FILE_ID_TXT, 'present': True, 'length': len(s), 'note': 'Excel file id present (not shown)'})
        except Exception as e:
            findings.append({'file': EXCEL_FILE_ID_TXT, 'present': True, 'note': f'Could not read file: {e}'})
    for fname in os.listdir('.'):
        lower = fname.lower()
        if any(k in lower for k in ('.env', 'secret', 'credential', 'key', '.pem', '.p12')):
            findings.append({'file': fname, 'present': True, 'note': 'Suspicious filename - check for secrets'})
    return findings

def compose_email_body_from_report(report_path):
    body_lines = []
    body_lines.append("SECRETS CHECK:")
    findings = scan_for_possible_secrets()
    if not findings:
        body_lines.append("- No obvious secret files detected in the workspace.")
    else:
        for f in findings:
            line = f"- File: {f.get('file')} — note: {f.get('note')}."
            if f.get('client_email'):
                line += f" client_email: {f.get('client_email')}."
            if f.get('has_private_key'):
                line += " Contains a private_key field (DO NOT share private key material)."
            if f.get('length') is not None:
                line += f" length: {f.get('length')} characters (value not shown)."
            body_lines.append(line)
        body_lines.append("\nIf any of the above files were accidentally committed to your repository: (1) rotate/disable keys, (2) remove the files from the repo (git filter-repo / bfg), (3) re-issue new credentials.")
    body_lines.append("\n--- REPORT CONTENT (pasted below) ---\n")
    body_lines.append(f"Run Report — {now_ist().strftime('%d %B %Y %H:%M')}")
    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            body_lines.append(f.read())
    except Exception as e:
        body_lines.append(f"⚠️ Could not read report file for email body: {e}")
    return "\n".join(body_lines)

# ---------------------------- Excel fetch from GDrive (preserve original) -
def fetch_excel_from_gdrive_bytes(excel_file_id, service_account_path):
    if not HAVE_GOOGLE_API:
        print("ℹ️ google-api-python-client or google-auth not available in this environment.")
        print("   Install dependencies in your workflow runner: pip install google-api-python-client google-auth-httplib2 google-auth")
        return None
    try:
        scopes = ['https://www.googleapis.com/auth/drive.readonly']
        creds = service_account.Credentials.from_service_account_file(service_account_path, scopes=scopes)
        drive_service = build('drive', 'v3', credentials=creds, cache_discovery=False)
        try:
            request = drive_service.files().get_media(fileId=excel_file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
                logd(f"Download progress: {int(status.progress() * 100)}%")
            fh.seek(0)
            return fh
        except Exception as e_bin:
            logd(f"files().get_media failed ({e_bin}), trying files().export (Sheets export)...")
            try:
                export_mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                request = drive_service.files().export_media(fileId=excel_file_id, mimeType=export_mime)
                fh = io.BytesIO()
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    logd(f"Export progress: {int(status.progress() * 100)}%")
                fh.seek(0)
                return fh
            except Exception as e_export:
                logd(f"files().export failed: {e_export}")
                return None
    except Exception as e:
        logd(f"Google Drive fetch failed: {e}")
        logd(traceback.format_exc())
        return None

# ---------------------------- Main updater (reordered & integrated) -----
def update_json_from_excel(excel_file_like, json_file, sheet_names, max_per_run=0, max_run_time_minutes=0):
    print(f"🚀 Running create_update_backup_delete.py — Version {SCRIPT_VERSION}")
    processed_total = 0
    start_time = now_ist()
    # Load existing JSON (if any)
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                old_objects = json.load(f)
        except Exception:
            print(f"⚠️ {json_file} invalid. Starting fresh.")
            old_objects = []
    else:
        old_objects = []
    old_by_id = {o['showID']: o for o in old_objects if 'showID' in o}
    merged_by_id = dict(old_by_id)
    report_changes_by_sheet = {}
    # import manual update summary if produced
    try:
        if os.path.exists(MANUAL_UPDATE_REPORT):
            with open(MANUAL_UPDATE_REPORT, 'r', encoding='utf-8') as muf:
                mu = json.load(muf)
            report_changes_by_sheet['Manual Update'] = mu
    except Exception:
        pass
    # 1) Process deletions first
    try:
        deleted_ids, deleting_not_found_initial = process_deletions(excel_file_like, json_file,
                                                                    report_changes_by_sheet.setdefault('Deleting Records', {}))
    except Exception as e:
        report_changes_by_sheet.setdefault('Deleting Records', {})['error'] = str(e)
        deleted_ids, deleting_not_found_initial = [], []
    # After deletions, reload the JSON file
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                old_objects = json.load(f)
        except Exception:
            old_objects = []
    old_by_id = {o['showID']: o for o in old_objects if 'showID' in o}
    merged_by_id = dict(old_by_id)
    deleting_found_in_sheets = set()
    progress = load_progress()
    overall_continued = False
    time_limit_seconds = max_run_time_minutes * 60 if max_run_time_minutes > 0 else None
    any_sheet_processed = False
    for s in sheet_names:
        report_changes = {}
        start_idx = int(progress.get(s, 0) or 0)
        try:
            items, processed, finished, next_start_idx = excel_to_objects(excel_file_like, s, merged_by_id, report_changes,
                                                                          start_index=start_idx,
                                                                          max_items=(max_per_run if max_per_run > 0 else None),
                                                                          time_limit_seconds=time_limit_seconds,
                                                                          deleted_ids_for_run=set(deleted_ids),
                                                                          deleting_not_found_initial=set(deleting_not_found_initial),
                                                                          deleting_found_in_sheets=deleting_found_in_sheets)
        except Exception as e:
            err = str(e)
            print(f"⚠️ Error processing {s}: {err}")
            report_changes['error'] = err
            items, processed, finished, next_start_idx = [], 0, True, start_idx
        for new_obj in items:
            sid = new_obj.get('showID')

            if sid in merged_by_id:
                old_obj = merged_by_id[sid]
                if old_obj != new_obj:
                    # compute changed fields and human-readable names
                    changed = []
                    for k in new_obj.keys():
                        if old_obj.get(k) != new_obj.get(k):
                            if k in ('updatedOn','updatedDetails','topRatings'):
                                continue
                            changed.append(human_readable_field(k))
                    new_obj['updatedOn'] = now_ist().strftime('%d %B %Y')
                    if changed:
                        new_obj['updatedDetails'] = f"{', '.join(changed)} Updated"
                    else:
                        new_obj['updatedDetails'] = 'Updated'
                    # Backup the old object before replacing (only when actual difference)
                    try:
                        backup_path = backup_before_modification(sid, old_obj)
                        if backup_path:
                            report_changes.setdefault('metadata_backups_created', []).append(f"💾 Backup Created: {backup_path}")
                    except Exception as e:
                        logd(f"backup attempt failed for {sid}: {e}")
                    # replace object
                    merged_by_id[sid] = new_obj
                else:
                    # no changes found -> mark as skipped
                    report_changes.setdefault('skipped', []).append(f"{old_obj.get('showName', 'Unknown')} ({old_obj.get('releasedYear', 'N/A')}) -> Unchanged (Skipped)")
            else:
                merged_by_id[sid] = new_obj

        if items:
            os.makedirs(BACKUP_DIR, exist_ok=True)
        # Per-sheet full-item backup skipped. Only individual BEFORE_* backups are created for modified records.
        if not any(k in report_changes for k in ('metadata_backups_created', 'images', 'updated')):
            report_changes.setdefault('info', []).append('No modified items to backup (full per-sheet backup skipped)')

        report_changes_by_sheet[s] = report_changes
        if processed > 0:
            any_sheet_processed = True
            processed_total += processed
        if not finished:
            progress[s] = next_start_idx
            overall_continued = True
        else:
            if s in progress:
                progress.pop(s, None)
        save_progress(progress)
    still_not_found = set(deleting_not_found_initial or []) - deleting_found_in_sheets
    merged = sorted(merged_by_id.values(), key=lambda x: x.get('showID', 0))

    # Ensure sitePriorityUsed exists on every object (fill defaults if missing)
    for _obj in merged:
        try:
            sp = _obj.get('sitePriorityUsed') or {}
            language = (_obj.get('nativeLanguage') or '').strip().lower()
            defaults = SITE_PRIORITY_BY_LANGUAGE.get(language, SITE_PRIORITY_BY_LANGUAGE.get('default', {}))
            for k in ('image','releaseDate','otherNames','duration','synopsis'):
                if k not in sp or not sp.get(k):
                    sp[k] = defaults.get(k)
            _obj['sitePriorityUsed'] = sp
        except Exception:
            pass
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(merged, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"⚠️ Could not write final {json_file}: {e}")
    os.makedirs(REPORTS_DIR, exist_ok=True)
    report_path = os.path.join(REPORTS_DIR, f"report_{SCRIPT_VERSION.replace(' ', '_').replace('(', '').replace(')', '')}_{filename_timestamp()}.txt")
    # cleanup metadata backups after image cleanup
    # existing behavior: cleanup deleted-data items and old images if scheduled run
    if SCHEDULED_RUN:
        cleanup_deleted_data()
    cutoff = datetime.now() - timedelta(days=KEEP_OLD_IMAGES_DAYS)
    if os.path.exists(DELETE_IMAGES_DIR):
        for fname in os.listdir(DELETE_IMAGES_DIR):
            path = os.path.join(DELETE_IMAGES_DIR, fname)
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(path))
                if mtime < cutoff:
                    os.remove(path)
            except Exception as e:
                print(f"⚠️ Could not cleanup old image {path}: {e}")
    # Now cleanup old metadata backups and record count for report
    removed_metadata_backups = cleanup_old_metadata_backups(METADATA_BACKUP_RETENTION_DAYS)
    write_report(report_changes_by_sheet, report_path, final_not_found_deletions=sorted(list(still_not_found)), start_time=start_time, end_time=now_ist(), metadata_backups_removed=removed_metadata_backups)
    print(f"✅ Report written -> {report_path}")
    # Compose email body
    email_body = compose_email_body_from_report(report_path)
    try:
        print('\n===EMAIL_BODY_START===')
        print(email_body)
        print('===EMAIL_BODY_END===\n')
    except Exception as e:
        print('⚠️ Failed printing email body to stdout:', e)
    status = {"continued": overall_continued, "timestamp": now_ist().strftime('%d %B %Y_%H.%M'), "processed_total": processed_total}
    try:
        with open(STATUS_JSON, 'w', encoding='utf-8') as sf:
            json.dump(status, sf, indent=2)
    except Exception as e:
        print(f"⚠️ Could not write status json: {e}")
    if processed_total == 0:
        print("⚠️ No records were processed in this run. Please check your Excel file and sheet names.")
        with open(os.path.join(REPORTS_DIR, "failure_reason.txt"), "w", encoding="utf-8") as ff:
            ff.write("No records processed. Check logs and the report.\n")
        return
    return

# ---------------------------- Entrypoint -----------------------------------
if __name__ == '__main__':
    if not (os.path.exists(EXCEL_FILE_ID_TXT) and os.path.exists(SERVICE_ACCOUNT_FILE)):
        print("❌ Missing GDrive credentials. Please set EXCEL_FILE_ID.txt and GDRIVE_SERVICE_ACCOUNT.json via GitHub secrets.")
        sys.exit(3)
    try:
        with open(EXCEL_FILE_ID_TXT, 'r', encoding='utf-8') as f:
            excel_id = f.read().strip()
    except Exception:
        excel_id = None
    if not excel_id:
        print("❌ EXCEL_FILE_ID.txt is empty or missing. Aborting gracefully.")
        sys.exit(0)
    _sheets_env = os.environ.get("SHEETS", "").strip()
    if _sheets_env:
        SHEETS = [s.strip() for s in _sheets_env.split(";") if s.strip()]
    else:
        SHEETS = ["Sheet1"]
    excel_bytes = fetch_excel_from_gdrive_bytes(excel_id, SERVICE_ACCOUNT_FILE)
    if excel_bytes is None:
        print("❌ Could not fetch Excel file from Google Drive. Exiting gracefully.")
        print("   Ensure the service account JSON and EXCEL_FILE_ID are correct, and required packages are installed.")
        sys.exit(0)
    excel_file_like = excel_bytes
    try:
        apply_manual_updates(excel_file_like, JSON_FILE)
    except Exception as e:
        logd(f"apply_manual_updates error: {e}")
    try:
        update_json_from_excel(excel_file_like, JSON_FILE, SHEETS, max_per_run=MAX_PER_RUN, max_run_time_minutes=MAX_RUN_TIME_MINUTES)
    except SystemExit:
        raise
    except Exception as e:
        print(f"❌ Unexpected error during update: {e}")
        logd(traceback.format_exc())
        sys.exit(1)
    print("All done.")


# ============================================================
# v2.3.3 Patch: Strict Site Priority Enforcement
# ============================================================
# For languages listed in SITE_PRIORITY_BY_LANGUAGE, do not fallback.
# If a site returns no data, log "Can't fetch ..." and skip fallback.

def fetch_with_strict_priority(native_lang, field, fetch_functions, SITE_PRIORITY_BY_LANGUAGE, report_changes):
    lang_key = str(native_lang).strip().lower()
    if lang_key in SITE_PRIORITY_BY_LANGUAGE:
        preferred_site = SITE_PRIORITY_BY_LANGUAGE[lang_key].get(field)
        if not preferred_site:
            return None, None

        fetch_func = fetch_functions.get(preferred_site)
        if not fetch_func:
            report_changes.setdefault('warnings', []).append(
                f"⚠️ No fetch function found for {preferred_site} ({field})"
            )
            return None, None

        try:
            result = fetch_func()
            if not result or all(not r for r in result if isinstance(r, (str, list))):
                report_changes.setdefault('warnings', []).append(
                    f"⚠️ Can't fetch {field} from {preferred_site}"
                )
                return None, preferred_site
            return result, preferred_site
        except Exception as e:
            report_changes.setdefault('warnings', []).append(
                f"⚠️ Error fetching {field} from {preferred_site}: {e}"
            )
            return None, preferred_site

    # If not in site priority list, fallback logic applies elsewhere
    return None, None



# =============================================================
# Helper: Determine reason for skipped/unchanged entries
# =============================================================
def determine_skip_reason(existing_obj, new_data, site_status=None, site_used=None):
    if site_status == "fetch_failed":
        return "Fetch skipped (site unavailable)"
    elif not new_data:
        return "No data fetched for this record"
    elif existing_obj == new_data:
        return "Identical data (no differences detected)"
    elif all(str(existing_obj.get(k)) == str(new_data.get(k)) for k in new_data.keys()):
        if site_used:
            return f"Already up to date from {site_used}"
        return "All fields already matched"
    else:
        return "No significant changes detected"
    