# ============================================================================
# Workflow: Excel → JSON Automation
# Purpose : Automate weekly + manual Excel → JSON updates, deletions, backups,
#           and run reports (TXT only) for "my-movie-database".
# ============================================================================

name: Excel → JSON Update

on:
  workflow_dispatch:
    inputs:
      MAX_RUN_TIME_MINUTES:
        description: 'Max run time in minutes for this run (0 = no time limit)'
        default: '300'
        required: false
      MAX_PER_RUN:
        description: 'Limit number of shows to process (0 = process all)'
        default: '0'
        required: false
  schedule:
    - cron: "30 18 * * 6"  # Scheduled: Every Sunday 00:00 AM IST (Saturday 18:30 UTC)

jobs:
  update-json:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Write Excel file ID
        run: echo "${{ secrets.EXCEL_FILE_ID }}" > EXCEL_FILE_ID.txt

      - name: Write Google Drive service account
        run: echo '${{ secrets.GDRIVE_SERVICE_ACCOUNT }}' > GDRIVE_SERVICE_ACCOUNT.json

      # ------------------ Run the updater once ------------------
      - name: Run seriesData updater and capture report path
        id: run_updater
        env:
          EVENT_NAME: ${{ github.event_name }}
          GITHUB_PAGES_URL: https://brucebanner001.github.io/my-movie-database
          MAX_PER_RUN: ${{ github.event.inputs.MAX_PER_RUN || 0 }}
          MAX_RUN_TIME_MINUTES: ${{ github.event.inputs.MAX_RUN_TIME_MINUTES || 300 }}
          KEEP_OLD_IMAGES_DAYS: 7
          SCHEDULED_RUN: ${{ github.event_name == 'schedule' && 'true' || 'false' }}
        run: |
          set +e

          # Run the updater and capture full stdout/stderr (do not fail the step on script exit)
          if [ -f create_update_backup_delete_always0.py ]; then
            output=$(python create_update_backup_delete_always0.py 2>&1 || true)
          elif [ -f create_update_backup_delete_final.py ]; then
            output=$(python create_update_backup_delete_final.py 2>&1 || true)
          else
            output=$(python create_update_backup_delete.py 2>&1 || true)
          fi
          printf '%s\n' "$output"

          # Primary method: look for dedicated REPORT_PATH=... and EMAIL_BODY_PATH=... lines printed by the script
          report_path=$(printf '%s\n' "$output" | grep -m1 '^REPORT_PATH=' | sed 's/^REPORT_PATH=//')
          email_body_path=$(printf '%s\n' "$output" | grep -m1 '^EMAIL_BODY_PATH=' | sed 's/^EMAIL_BODY_PATH=//')

          # Fallback for report: older scripts printed "Report written → <path>"
          if [ -z "$report_path" ] || [ ! -f "$report_path" ]; then
            report_path=$(printf '%s\n' "$output" | awk -F'→' '/Report written/ {gsub(/^[ \t]+|[ \t]+$/, "", $2); print $2; exit}')
          fi

          # Final fallback for report: use most recent report file on disk
          if [ -z "$report_path" ] || [ ! -f "$report_path" ]; then
            report_path=$(ls -t reports/report_*.txt 2>/dev/null | head -n1 || true)
          fi

          # Fallback for email body: most recent email_body file if not provided by the script
          if [ -z "$email_body_path" ] || [ ! -f "$email_body_path" ]; then
            email_body_path=$(ls -t reports/email_body_*.txt 2>/dev/null | head -n1 || true)
          fi

          # Export to workflow outputs for downstream steps
          if [ -z "$report_path" ]; then
            echo "report_path=" >> $GITHUB_OUTPUT
          else
            echo "report_path=$report_path" >> $GITHUB_OUTPUT
          fi

          if [ -z "$email_body_path" ]; then
            echo "email_body_path=" >> $GITHUB_OUTPUT
          else
            echo "email_body_path=$email_body_path" >> $GITHUB_OUTPUT
          fi

          # Also export an email subject
          if [ "${EVENT_NAME:-}" = "schedule" ]; then
            prefix="[Automatic]"
          else
            prefix="[Manual]"
          fi
          subj_date=$(date -u "+%d %B %Y %H%M")
          echo "email_subject=$prefix Workflow $subj_date Report" >> $GITHUB_OUTPUT

      - name: Commit & push changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add seriesData.json images/ backups/ reports/ deleted-data/ old-images/
          git commit -m "Automated update: $(date -u '+%Y-%m-%d %H:%M:%S')" || echo "No changes to commit"
          git push

      - name: Upload reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: run-reports
          path: reports/

      # ------------------ Compose consolidated email ------------------
      - name: Compose consolidated email body
        id: compose_email
        run: |
          set +e

          # Prefer email body generated by the Python script (via step output email_body_path)
          email_file="${{ steps.run_updater.outputs.email_body_path }}"
          if [ -n "$email_file" ] && [ -f "$email_file" ]; then
            cp "$email_file" email_body.txt
          else
            # Fallback: build manually from the report file
            report="${{ steps.run_updater.outputs.report_path }}"
            if [ -z "$report" ] || [ ! -f "$report" ]; then
              report=$(ls -t reports/report_*.txt 2>/dev/null | head -n1 || true)
            fi

            {
              echo "SECRETS CHECK:"
              if [ -s reports/secrets_report.txt ]; then
                echo "- Gitleaks detected potential secrets. (Details below)"
                sed 's/%/%%/g' reports/secrets_report.txt
              else
                echo "- No obvious secrets detected."
              fi
              echo ""
              echo "--- REPORT CONTENT (pasted below) ---"
              if [ -n "$report" ] && [ -f "$report" ]; then
                sed 's/%/%%/g' "$report"
              else
                echo "⚠️ Report file not found."
              fi
            } > email_body.txt
          fi

          # Export the path to the created email body file for the mail action
          echo "email_body_path=email_body.txt" >> $GITHUB_OUTPUT

      - name: Collect Email Body File
        id: collect_email
        run: |
          EMAIL_FILE=$(ls reports/email_body_*.txt | tail -n1)
          echo "EMAIL_FILE=$EMAIL_FILE" >> $GITHUB_ENV
      - name: Send consolidated report email
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: ${{ steps.run_updater.outputs.email_subject }}
          to: ${{ secrets.NOTIFY_EMAIL }}
          from: GitHub Actions <${{ secrets.SMTP_USERNAME }}>
          body_file: ${{ env.EMAIL_FILE }}

      # ------------------ Cleanup old backups (older than 15 days) ------------------
      - name: Cleanup old backups (older than 15 days)
        run: |
          find backups/ -type f -name "backup_*.json" -mtime +15 -print -delete

      # ------------------ Cleanup old reports (older than 15 days) ------------------
      - name: Cleanup old reports (older than 15 days)
        run: |
          find reports/ -type f -name "report_*.txt" -mtime +15 -print -delete

      # ------------------ Cleanup old deleted-data (older than 15 days) ------------------
      - name: Cleanup old deleted-data (older than 15 days)
        run: |
          find deleted-data/ -type f -name "*.json" -mtime +15 -print -delete

      # ------------------ Cleanup old-images (older than 15 days) ------------------
      - name: Cleanup old-images (older than 15 days)
        run: |
          find old-images/ -type f -mtime +15 -print -delete